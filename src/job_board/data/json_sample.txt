{
  "status": "OK",
  "request_id": "e1edb283-81fc-49cb-ac33-93ee00907d80",
  "data": [
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "KSNT Jobs",
      "job_id": "jLxl4NX4djMAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.ksnt.com/jobs/staff-data-engineer-remote-usa-tampa-florida/906541334-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612680,
      "job_posted_at_datetime_utc": "2023-02-05T15:58:00.000Z",
      "job_city": "Tampa",
      "job_state": "FL",
      "job_country": "US",
      "job_latitude": 27.950575,
      "job_longitude": -82.45718,
      "job_benefits": [
        "retirement_savings",
        "health_insurance",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=0&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=jLxl4NX4djMAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "AURA Technologies, LLC",
      "employer_logo": "https://media.licdn.com/dms/image/C4D0BAQETPxFDUSE3qA/company-logo_200_200/0/1624473059742?e=2147483647&v=beta&t=aQ0F1h2JOoWPgUjLWU4o8JOIej_MS4U73GfvPb8JsMo",
      "employer_website": "http://aura.company",
      "job_publisher": "LinkedIn",
      "job_id": "aPhawJQYWAEAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Senior Data Engineer",
      "job_apply_link": "https://www.linkedin.com/jobs/view/senior-data-engineer-at-aura-technologies-llc-3470628739",
      "job_description": "AURA TECHNOLOGIES, LLC (AURA) is an advanced research and development (R&D) and technology company creating game-changing innovations for the US Department of Defense in Artificial Intelligence (AI) and in other systems-level implementation of cutting-edge technology. We are creating advanced intelligent power systems for the US Army; unconventional tactical lights for the US Marines; revolutionary satellite manufacturing for the US Air Force; and a range of AI platforms for DoD implementation. AURA partners with some of the best companies in the world, such as Boeing, Northrop Grumman, and Lockheed Martin. We also collaborate with the best and brightest at our nation’s universities, including Georgia Tech and NC State University.\n\nIf you are a smart, capable, and talented individual who possesses high integrity, thrives in a fast-paced environment, wants to chart your own course based on your capabilities, and is willing to be accountable for failures and successes, then continue reading because you may be the ideal candidate to join our growing R&D business.\n\nAURA has an immediate opening for a full-time Senior Data Engineer (remote).\n\nESSENTIAL DUTIES AND RESPONSIBILITIES:\n\nYou will work side-by-side with other team members to provide technical direction and recommendations on how to: standardize, normalize, and format data; extract valuable features from large data sets; develop data pipelines and write preprocessing algorithms; and train others to use and maintain developed tools/applications.\n• Work on projects involving time-series data, image recognition, computer vision, and geometric shape modeling.\n• Preprocess and extract features from sensor-based, three-dimensional, and visual data.\n• Develop applications to sustain data pipelines, interact with databases, and ensure uptime.\n• Interact with big data applications for visual, video, and geometric information.\n• Collaborate closely with data scientists to ensure that data throughput and preprocessing requirements are met.\n• Work in a team environment to collaborate with coworkers, partners, and clients to produce an integrated solution. Given the demanding, diverse, and fast-paced environment, the Senior Data Engineer must also possess exceptional attention to detail.\n\nWORK EXPERIENCE, EDUCATION & TECHNICAL REQUIREMENTS:\n\nMinimum Years of Work Experience:\n\nMinimum of 3 years of experience in a data engineer, cloud engineer, database management, or similar role (advanced degrees may be substituted for experience in the case of a qualified candidate)\n\nMinimum Education:\n\nMasters degree in Computer Science, Engineering, Data Science, Statistics, or a related technical degree\n\nMinimum Technical Requirements:\n• At least 3 years of experience (in a professional setting) developing data pipelines and preprocessing algorithms.\n• Expertise in one or more structured database management system types/vendors such as MySQL, Oracle, or SQLite.\n• Expertise in one or more unstructured data management systems.\n• Programming expertise in one or more scripting languages, such as Python or R.\n• Awareness of big data design for cloud applications such as AWS or Azure, including design, provisioning, and tuning.\n\nand EITHER:\n• At least 1-2 years of practical experience with storing and preprocessing visual data, including the development of data pipelines for large image datasets. Past experience working with large open-source visual datasets such as OpenImages, ImageNet, or MS-COCO is preferred.\n• At least 1-2 years of practical expertise working with three-dimensional geometric data, such as computer-aided design (CAD) files, bounding boxes, and shape data. Past experience working with large open-source 3D datasets such as ShapeNet, ModelNet, or ABC is preferred.\n\nPREFERRED REQUIREMENTS\n• Prior DoD or military experience\n• Secret clearance (or be able to obtain once employed)\n• Doctorate degree in Computer Science, Engineering, Data Science, Statistics, or a related technical degree\n\nADDITIONAL REQUIREMENTS:\n• Candidates must not now or in the future require sponsorship for employment visa status\n• SECURITY CLEARANCE: The successful candidate may already have, or be able to obtain once employed by AURA, a US Government Secret-level clearance. The successful candidate must maintain the clearance during the duration of their employment, which is a requirement of this position. The US Government generally only grants clearances to US citizens who are free from major criminal convictions. Individuals not meeting these two criteria will most likely not be granted a US Government security clearance.\n\nBENEFITS:\n• 401(k) Safe Harbor Contribution\n• Flexible schedule\n• Health insurance\n• Discretionary Leave\n• 14 Paid holidays\n\nTO APPLY FOR THIS POSITION:\n\nSubmit your resume/CV in PDF format via instructions at the following link: https://auratech.bamboohr.com/careers/75?source=aWQ9NA%3D%3D\n\nNo phone calls after submission. We will let candidates know via automated reply that we have received their resumes and will contact them if there is a good fit after the closing date for this job.\n\nAURA Technologies, LLC is an Equal Opportunity Employer and affirmative action employer of veterans protected under the Vietnam Era Veterans’ Readjustment Assistant Act (VEVRAA). We are a Drug Free Workplace and thus, all job offers are contingent on successful criminal background check and drug screen. As a US Federal Contractor, AURA uses the Department of Homeland Security e-Verify system to determine eligibility to legally work in the United States. Most of AURA’s work is for the federal government, and federal regulations may in the future require AURA’s employees to be fully vaccinated against the COVID-19 virus.\n\nWrite a carefully crafted, well-written cover letter that elaborates on your interest in this position and why you think you are the best candidate for the job. Submit your cover letter, CV and three professional references in PDF format ONLY via BambooHR.\n\nAny attachments must be in PDF format or will not be opened due to virus concerns. No phone calls after submission. We will let candidates know via automated reply that we have received their resumes and will contact them if there is a good fit after the closing date for this job.",
      "job_is_remote": true,
      "job_posted_at_timestamp": 1675604050,
      "job_posted_at_datetime_utc": "2023-02-05T13:34:10.000Z",
      "job_city": null,
      "job_state": null,
      "job_country": "US",
      "job_latitude": 37.09024,
      "job_longitude": -95.71289,
      "job_benefits": [
        "health_insurance",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=0&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=aPhawJQYWAEAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-08-04T13:33:59.000Z",
      "job_offer_expiration_timestamp": 1691156039,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 36,
        "experience_mentioned": true,
        "experience_preferred": true
      },
      "job_required_skills": [
        "Data Pipelines",
        "Scripting",
        "Computer Science",
        "Big Data",
        "Image Recognition",
        "Cloud Applications"
      ],
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": true,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Minimum of 3 years of experience in a data engineer, cloud engineer, database management, or similar role (advanced degrees may be substituted for experience in the case of a qualified candidate)",
          "Masters degree in Computer Science, Engineering, Data Science, Statistics, or a related technical degree",
          "At least 3 years of experience (in a professional setting) developing data pipelines and preprocessing algorithms",
          "Expertise in one or more structured database management system types/vendors such as MySQL, Oracle, or SQLite",
          "Expertise in one or more unstructured data management systems",
          "Programming expertise in one or more scripting languages, such as Python or R",
          "Awareness of big data design for cloud applications such as AWS or Azure, including design, provisioning, and tuning",
          "At least 1-2 years of practical experience with storing and preprocessing visual data, including the development of data pipelines for large image datasets",
          "Candidates must not now or in the future require sponsorship for employment visa status",
          "SECURITY CLEARANCE: The successful candidate may already have, or be able to obtain once employed by AURA, a US Government Secret-level clearance",
          "The successful candidate must maintain the clearance during the duration of their employment, which is a requirement of this position"
        ],
        "Responsibilities": [
          "You will work side-by-side with other team members to provide technical direction and recommendations on how to: standardize, normalize, and format data; extract valuable features from large data sets; develop data pipelines and write preprocessing algorithms; and train others to use and maintain developed tools/applications",
          "Work on projects involving time-series data, image recognition, computer vision, and geometric shape modeling",
          "Preprocess and extract features from sensor-based, three-dimensional, and visual data",
          "Develop applications to sustain data pipelines, interact with databases, and ensure uptime",
          "Interact with big data applications for visual, video, and geometric information",
          "Collaborate closely with data scientists to ensure that data throughput and preprocessing requirements are met",
          "Work in a team environment to collaborate with coworkers, partners, and clients to produce an integrated solution",
          "Given the demanding, diverse, and fast-paced environment, the Senior Data Engineer must also possess exceptional attention to detail"
        ],
        "Benefits": [
          "401(k) Safe Harbor Contribution",
          "Flexible schedule",
          "Health insurance",
          "Discretionary Leave",
          "14 Paid holidays"
        ]
      }
    },
    {
      "employer_name": "Experian",
      "employer_logo": "https://assets.experiancs.com/images/logo-experian.svg?hs=2c108564c86d0b65336076616b44b886",
      "employer_website": "http://www.experian.com",
      "job_publisher": "Nexxt",
      "job_id": "C0R0MT_j-noAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer (REMOTE USA ONLY)",
      "job_apply_link": "https://www.nexxt.com/jobs/data-engineer-remote-usa-only-costa-mesa-ca-2437008752-job.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic&aff=2ED44C72-8FD2-4B5D-BC54-2F623E88BE26",
      "job_description": "Experian is the world's leading global information services company. During life's big moments - from buying a home or a car, to sending a child to college, to growing a business by connecting with new customers - we empower consumers and our clients to manage their data with confidence. We help individuals to take financial control and access financial services, businesses to make smarter decisions and thrive, lenders to lend more responsibly, and organizations to prevent identity fraud and crime.\n\nWe have 20,000 people operating across 44 countries and every day we're investing in new technologies, talented people, and innovation to help all our clients maximize every opportunity\n\nWe are thrilled to share that FORTUNE has named Experian one of the 100 Best Companies to Work For. In addition, for the last five years we've been name in the top 100 \"World's Most Innovative Companies\" by Forbes Magazine.\n\nThis role is responsible for supporting data manipulation and reporting, quality assurance, documentation and communication of internal/client projects. Applies knowledge of data science concepts and techniques to help develop and validate data attributes/models for business data acquisition, aggregation, and dissemination. This position will provide direction and supervise a team of Data Engineers.\n• Independently lead and execute analytical projects based on consumer credit attributes or underlying credit data.\n• Expert client consulting experience, providing dedicated client support and acting as the liaison between the product delivery team and clients.\n• Understand the client's existing product usage and use cases to develop and recommend a strategic plan based on consumer credit attributes and credit data to support clients' key objectives.\n• Ability to gather detailed requirements from the clients, navigate and agree on the product options and create a custom project plan along with timeline and level of effort.\n• Experience in leading teams and willingness to drive the team to accomplish project goals.\n• Interpret data specifications, extract, formulate and manipulate data to create the structured samples (attributes and models) necessary to address business requirement\n• Test and validate the structured samples to ensure the highest quality and efficiency\n• Develop alternative/creative solutions that better utilize Experian and client data and produce new insights\n• Manage internal/external projects to meet project commitments within agreed schedule with assistance\n• Manage multiple tasks simultaneously without compromising the quality of the final deliverables.\n• Communicate in a clear and professional way with client and/or other departments/divisions within Experian to help ensure resolution of client questions, capturing of client requirements and implementation of project work\n• Document projects worked on and technical manuals for tools developed\n• Plans, staffs, reviews and organizes subordinate supervisor or staff assignments for a team within a department. Provides leadership, coaching, guidance, training and staff development.\n• Responsible for staff salary planning, personnel actions, and performance management process.\n\nMust be able to perform the following with minimal guidance:\n• Conduct exploratory analysis using in statistical packages (SAS or R) or other languages\n• Perform advanced statistical analysis using statistical packages or other languages desired\n• Excellent communication skills, including proficiency in writing reports and presenting technical work\n• Strong personal planning and time management skills and interpersonal skills\n• Develop processes in Excel VBA\n• Write complex SQL statements against terabyte+ size datasets\n• Works well in a distributed team environment\n• Feature engineering experience\n• Good leadership skills\n\nNice to have/preferred Experience:\n• (Risk) Modeling experience\n• Advanced knowledge of consumer credit industry, data and attributes\n• A willingness to train and mentor the team on the consulting aspect of Attribute products\n• AWS S3, PySpark\n\nMust haves:\n• Hold an advance degree in Data Science, Computer Science, Engineering, Economics or Statistics\n• 10+ years of experience performing analytical work based on large relational databases, using at least 2 languages such as Python, SAS, SQL, C, C++ or Java\n• 2-4 years supervisory experience\n\nWhy choose us?\n\nOur colleagues' health and wellbeing are a top priority for us, that's why our reward, benefits and wellbeing programs are designed so you can come to work feeling your very best self. Our benefits focus on health, money, and lifestyle so you can tailor your benefits to your own personal needs. Whether it's your physical and mental wellness, getting to work or preparing for the next big milestone in your life, we have a range of flexible options to have you covered!\n• To learn more about our culture and what it's really like to work here, check out our interactive guide here: ~~~\n• Could this be the role for you? Apply now to start your journey with Experian.\n• To learn more about our culture and what it's really like to work here, check out our LinkedIn and social media channels using the hashtags #UniquelyExperian\n• Youtube video link: ~~~\n• Experian Careers - Creating a better tomorrow together\n• Find out what its like to work for Experian by clicking here (~~~/)\n\nLI-REMOTE\n\nExperian is proud to be an Equal Opportunity and Affirmative Action employer. Our goal is to create a thriving, inclusive and diverse team where people love their work and love working together. We believe that diversity, equity and inclusion is essential to our purpose of creating a better tomorrow. We value the uniqueness of every individual and want you to bring your whole, authentic self to work. For us, this is The Power of YOU (~~~) and and it reflects what we believe. See our DEI work in action (~~~/) !\n\nPlease contact us at ~~~ to request the salary range of this position (please include the exact Job Title as it reads above in your email). In addition to a competitive base salary and variable pay opportunity, Experian offers a comprehensive benefits package including health, life and disability insurance, generous paid time off including 12 company paid holidays and parental and family care leave, an employee stock purchase plan and a 401(k) plan with a company match.\n\nExperian Careers - Creating a better tomorrow together\n\nFind out what its like to work for Experian by clicking here (~~~/)",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675617298,
      "job_posted_at_datetime_utc": "2023-02-05T17:14:58.000Z",
      "job_city": "Costa Mesa",
      "job_state": "CA",
      "job_country": "US",
      "job_latitude": 33.663845,
      "job_longitude": -117.90474,
      "job_benefits": [
        "dental_coverage",
        "retirement_savings",
        "health_insurance",
        "paid_time_off"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=0&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=C0R0MT_j-noAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 120,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": false,
        "degree_preferred": true,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Experience in leading teams and willingness to drive the team to accomplish project goals",
          "Conduct exploratory analysis using in statistical packages (SAS or R) or other languages",
          "Excellent communication skills, including proficiency in writing reports and presenting technical work",
          "Strong personal planning and time management skills and interpersonal skills",
          "Develop processes in Excel VBA",
          "Write complex SQL statements against terabyte+ size datasets",
          "Works well in a distributed team environment",
          "Feature engineering experience",
          "Good leadership skills",
          "Hold an advance degree in Data Science, Computer Science, Engineering, Economics or Statistics",
          "10+ years of experience performing analytical work based on large relational databases, using at least 2 languages such as Python, SAS, SQL, C, C++ or Java",
          "2-4 years supervisory experience"
        ],
        "Responsibilities": [
          "This role is responsible for supporting data manipulation and reporting, quality assurance, documentation and communication of internal/client projects",
          "Applies knowledge of data science concepts and techniques to help develop and validate data attributes/models for business data acquisition, aggregation, and dissemination",
          "This position will provide direction and supervise a team of Data Engineers",
          "Independently lead and execute analytical projects based on consumer credit attributes or underlying credit data",
          "Expert client consulting experience, providing dedicated client support and acting as the liaison between the product delivery team and clients",
          "Understand the client's existing product usage and use cases to develop and recommend a strategic plan based on consumer credit attributes and credit data to support clients' key objectives",
          "Interpret data specifications, extract, formulate and manipulate data to create the structured samples (attributes and models) necessary to address business requirement",
          "Test and validate the structured samples to ensure the highest quality and efficiency",
          "Develop alternative/creative solutions that better utilize Experian and client data and produce new insights",
          "Manage internal/external projects to meet project commitments within agreed schedule with assistance",
          "Manage multiple tasks simultaneously without compromising the quality of the final deliverables",
          "Communicate in a clear and professional way with client and/or other departments/divisions within Experian to help ensure resolution of client questions, capturing of client requirements and implementation of project work",
          "Document projects worked on and technical manuals for tools developed",
          "Plans, staffs, reviews and organizes subordinate supervisor or staff assignments for a team within a department",
          "Provides leadership, coaching, guidance, training and staff development",
          "Responsible for staff salary planning, personnel actions, and performance management process"
        ],
        "Benefits": [
          "Our benefits focus on health, money, and lifestyle so you can tailor your benefits to your own personal needs",
          "Whether it's your physical and mental wellness, getting to work or preparing for the next big milestone in your life, we have a range of flexible options to have you covered!",
          "In addition to a competitive base salary and variable pay opportunity, Experian offers a comprehensive benefits package including health, life and disability insurance, generous paid time off including 12 company paid holidays and parental and family care leave, an employee stock purchase plan and a 401(k) plan with a company match"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": "https://www.digimarc.com/sites/default/files/content/paragraphs/col-with-img/2022-09/Digimarc-logo-stacked-black.png",
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "WANE Jobs",
      "job_id": "hQyZsIzgdxwAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.wane.com/jobs/staff-data-engineer-remote-usa-portland-oregon/906542352-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612699,
      "job_posted_at_datetime_utc": "2023-02-05T15:58:19.000Z",
      "job_city": "Portland",
      "job_state": "OR",
      "job_country": "US",
      "job_latitude": 45.515232,
      "job_longitude": -122.67838,
      "job_benefits": [
        "retirement_savings",
        "health_insurance",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=0&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=hQyZsIzgdxwAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Jobs Market",
      "job_id": "kUlJrSBj-G4AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobsmarket.com/jobs/staff-data-engineer-remote-usa-seattle-washington/906524268-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612428,
      "job_posted_at_datetime_utc": "2023-02-05T15:53:48.000Z",
      "job_city": "Seattle",
      "job_state": "WA",
      "job_country": "US",
      "job_latitude": 47.60621,
      "job_longitude": -122.33207,
      "job_benefits": [
        "dental_coverage",
        "health_insurance",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=0&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=kUlJrSBj-G4AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T15:53:48.000Z",
      "job_offer_expiration_timestamp": 1678204428,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "RABC Group",
      "employer_logo": null,
      "employer_website": "http://rabc.sg",
      "job_publisher": "SimplyHired",
      "job_id": "Z_afm7qHzbQAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer",
      "job_apply_link": "https://www.simplyhired.com/job/2EX6ubSFk9HE3hI23ZbivA1hRrrc77_5pC4L6FgpwMpAfogOPx64yg",
      "job_description": "We are a growing consulting organization that focuses on fintech and banking. The candidate will have a strong background in software and data engineering, and a passion for working with large diverse data sets.\n\nWhat you’ll do:\n\nCollaborate with and across analytics and business teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\n\nDesign and implement data pipelines, architectures and data sets to support business and data analytics needs.\n\nWork with cross-functional teams to understand and define data requirements, and translate them into technical solutions.\n\nBuild, optimize, and maintain data storage and processing systems, including data warehousing, ETL, and data lake technologies.\n\nQualifications:\n• 3-5 years of experience in data engineering or software development.\n• Strong hands-on experience with data warehousing, ETL, and data lake technologies.\n• Experience with SQL and at least one programming language such as Python, Java or Scala.\n• Familiarity with big data technologies such as Hadoop, Spark, and Hive.\n• Experience with cloud-based data storage and processing solutions, such as AWS, Azure, or GCP.\n• Strong understanding of data modeling, data warehousing and data governance.\n• Strong problem solving, analytical, and communication skills.\n• Experience with Agile software development methodologies.\n• Experience with DevOps methodologies and tools (e.g., Git, Artifactory, etc.).\n\nJob Types: Full-time, Permanent\n\nPay: $75,000.00 - $132,000.00 per year\n\nExperience level:\n• 3 years\n• 4 years\n\nSchedule:\n• 8 hour shift\n\nExperience:\n• SQL: 4 years (Preferred)\n\nWork Location: Remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675658459,
      "job_posted_at_datetime_utc": "2023-02-06T04:40:59.000Z",
      "job_city": null,
      "job_state": null,
      "job_country": "US",
      "job_latitude": 37.09024,
      "job_longitude": -95.71289,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=0&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=Z_afm7qHzbQAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 36,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": false,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "3-5 years of experience in data engineering or software development",
          "Strong hands-on experience with data warehousing, ETL, and data lake technologies",
          "Experience with SQL and at least one programming language such as Python, Java or Scala",
          "Familiarity with big data technologies such as Hadoop, Spark, and Hive",
          "Experience with cloud-based data storage and processing solutions, such as AWS, Azure, or GCP",
          "Strong understanding of data modeling, data warehousing and data governance",
          "Strong problem solving, analytical, and communication skills",
          "Experience with Agile software development methodologies",
          "Experience with DevOps methodologies and tools (e.g., Git, Artifactory, etc.)",
          "3 years",
          "4 years"
        ],
        "Responsibilities": [
          "Collaborate with and across analytics and business teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies",
          "Design and implement data pipelines, architectures and data sets to support business and data analytics needs",
          "Work with cross-functional teams to understand and define data requirements, and translate them into technical solutions",
          "Build, optimize, and maintain data storage and processing systems, including data warehousing, ETL, and data lake technologies"
        ],
        "Benefits": [
          "Pay: $75,000.00 - $132,000.00 per year"
        ]
      }
    },
    {
      "employer_name": "LexisNexis",
      "employer_logo": "https://www.lexisnexis.com/images/common/svg/ln-logo.svg",
      "employer_website": "http://www.lexisnexis.com/en-us/home.page",
      "job_publisher": "LinkedIn",
      "job_id": "yJ371O74y6QAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer III",
      "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-iii-at-lexisnexis-3463532321",
      "job_description": "Nexis Solutions, a business unit of LexisNexis, is seeking a Data Engineer to perform research, design, and data engineering assignments collaborating with other data engineers and scientists to focus on search and the creation of next-generation relevance techniques. The Data Engineer role collaborates across all teams and will help us tackle new problems as we continue to push technology forward.\n\nACCOUNTABILITIES:\n• Own specific set or group of data transfers in various capacities including collection setup, data transfer setup, contributor/customer setup, etc.\n• Interface with other technical personnel or team members to document, interpret, and finalize requirements.\n• Produce code that is efficient, repeatable, without defects, and adherent to best practices such as naming conventions, encapsulation, etc.\n• Write and review portions of detailed specifications for the development of data components.\n• Complete data engineering bug fixes and issues, researching and identifying root causes as appropriate.\n• Identify opportunities to apply automation or other tools to improve effectiveness or efficiency.\n• Innovate process improvements that enable efficient delivery and maintenance.\n• Oversee specific database management ensuring structure and dataflow adheres to department standards.\n• Utilize various data workflow management and analysis tools.\n• Good level of intimacy within a specific data content area.\n• Participate in process improvement and compliance to successfully and consistently deliver high quality services on time, and to specification, resulting in flexibility to react quickly to changes in priorities or circumstances to meet business needs.\n• Complete simple data engineering bug fixes and resolve technical issues as necessary.\n• Work closely with other engineering team members to understand data and translate requirements.\n• Operate in various development environments (Agile, Kanban, etc.) while collaborating with key stakeholders.\n• Participate in project reviews.\n\nRESPONSIBILITIES\n• Build and scale data infrastructure that powers batch and real-time data processing of billions of records\n• Design and build scalable data ingestion and enrichment pipelines (machine learning inference)\n• Automate and handle life cycle of the systems and platforms that process our data\n• Provide visibility into the health of our data platform (comprehensive view of data flow, resources usage, data lineage, etc.)\n• Mentor teammates on algorithms, data structures, design patterns\n• Evolve maturity of our data quality and monitoring systems\n• Implement development processes, coding best practices, and code reviews\n\nREQUIRED QUALIFICATIONS\n• Master’s in computer science or equivalent practical experience\n• Expertise with the following programming language: SQL, Python, Scala, or Java\n• Experience working with data technologies that power analytics (Spark required plus Hadoop or Presto or Redshift or Snowflake)\n• Experience working with Docker and Kubernetes\n• Experience with any cloud solutions (AWS, GCP, Azure)\n• Relational Database Modeling\n• NoSql and Document Oriented models (familiar with)\n• Extract Transform Load tools (or ELT) tools and concepts\n• Basic Visualization skills – Pivot, Line and Plot Charts in Excel, PowerBI, or similar\n\nPREFERRED QUALIFICATIONS\n• Familiar with core text analytics tasks like classification, clustering, LDA, NER, and Relationship Extraction.\n• Knowledge Bases, Taxonomies, Graph Databases, knowledge of Open Data sets like Wikidata, Yago, DBPedia, etc.\n• Graph Algorithms (shortest path, connected components, message passing, PageRank, triangle counting, etc.)\n• Visualization / dashboard tools like Tableau, PowerBI, and other reporting tools\n• Information Retrieval (search) esp. on Elastic or Solr",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612947,
      "job_posted_at_datetime_utc": "2023-02-05T16:02:27.000Z",
      "job_city": "Raleigh",
      "job_state": "NC",
      "job_country": "US",
      "job_latitude": 35.77959,
      "job_longitude": -78.638176,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=0&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=yJ371O74y6QAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T16:02:27.000Z",
      "job_offer_expiration_timestamp": 1678204947,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": true
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": true,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Master’s in computer science or equivalent practical experience",
          "Expertise with the following programming language: SQL, Python, Scala, or Java",
          "Experience working with data technologies that power analytics (Spark required plus Hadoop or Presto or Redshift or Snowflake)",
          "Experience working with Docker and Kubernetes",
          "Experience with any cloud solutions (AWS, GCP, Azure)",
          "Relational Database Modeling",
          "NoSql and Document Oriented models (familiar with)",
          "Extract Transform Load tools (or ELT) tools and concepts",
          "Basic Visualization skills – Pivot, Line and Plot Charts in Excel, PowerBI, or similar"
        ],
        "Responsibilities": [
          "The Data Engineer role collaborates across all teams and will help us tackle new problems as we continue to push technology forward",
          "Own specific set or group of data transfers in various capacities including collection setup, data transfer setup, contributor/customer setup, etc",
          "Interface with other technical personnel or team members to document, interpret, and finalize requirements",
          "Produce code that is efficient, repeatable, without defects, and adherent to best practices such as naming conventions, encapsulation, etc",
          "Write and review portions of detailed specifications for the development of data components",
          "Complete data engineering bug fixes and issues, researching and identifying root causes as appropriate",
          "Identify opportunities to apply automation or other tools to improve effectiveness or efficiency",
          "Innovate process improvements that enable efficient delivery and maintenance",
          "Oversee specific database management ensuring structure and dataflow adheres to department standards",
          "Utilize various data workflow management and analysis tools",
          "Good level of intimacy within a specific data content area",
          "Participate in process improvement and compliance to successfully and consistently deliver high quality services on time, and to specification, resulting in flexibility to react quickly to changes in priorities or circumstances to meet business needs",
          "Complete simple data engineering bug fixes and resolve technical issues as necessary",
          "Work closely with other engineering team members to understand data and translate requirements",
          "Operate in various development environments (Agile, Kanban, etc.)",
          "while collaborating with key stakeholders",
          "Participate in project reviews",
          "Build and scale data infrastructure that powers batch and real-time data processing of billions of records",
          "Design and build scalable data ingestion and enrichment pipelines (machine learning inference)",
          "Automate and handle life cycle of the systems and platforms that process our data",
          "Provide visibility into the health of our data platform (comprehensive view of data flow, resources usage, data lineage, etc.)",
          "Mentor teammates on algorithms, data structures, design patterns",
          "Evolve maturity of our data quality and monitoring systems",
          "Implement development processes, coding best practices, and code reviews"
        ]
      }
    },
    {
      "employer_name": "Robert Half",
      "employer_logo": "https://www.roberthalf.com/sites/default/files/rh-logo-rgb.jpg",
      "employer_website": "http://www.rhi.com",
      "job_publisher": "Recruit.net",
      "job_id": "awuDPbDnDKoAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer",
      "job_apply_link": "https://www.recruit.net/job/data-engineer-jobs/02B56A38E1766E20",
      "job_description": "Ref ID: 00320-0012572965\nClassification: Big Data Engineer\nCompensation: $120000.00 to $180000.00 yearly\n\nData Engineer\n\n100% REMOTE in the US and Canada, however must work core hours of Pacific Time Zone (West Coast Hours).\n\n$120K - $180K + eligible for stock options\n\nHealth, Dental, 401K, 20 days PTO\n\nKey Skills: AWS services including S3, Redshift and Kinesis Firehose\n\nData is very important to us. We are seeking an experienced Data Engineer that can hit the ground running, working across our data stack. Our core products and insights are backed by our data warehousing services, so reliability and high availability in these services is critical. We make data-driven decisions to improve our products, develop new features, and enhance existing functionality. The Data Engineering team is responsible for sourcing, ingesting, and processing customer-sourced data into our data platforms for use by other parts of the business. If you're looking for an exciting REMOTE opportunity to join a growing startup, please apply today!",
      "job_is_remote": true,
      "job_posted_at_timestamp": 1675641600,
      "job_posted_at_datetime_utc": "2023-02-06T00:00:00.000Z",
      "job_city": "Los Angeles",
      "job_state": "CA",
      "job_country": "US",
      "job_latitude": 34.052235,
      "job_longitude": -118.24368,
      "job_benefits": [
        "health_insurance",
        "paid_time_off",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=0&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=awuDPbDnDKoAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-08T00:00:00.000Z",
      "job_offer_expiration_timestamp": 1678233600,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": false,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Key Skills: AWS services including S3, Redshift and Kinesis Firehose"
        ],
        "Benefits": [
          "Compensation: $120000.00 to $180000.00 yearly",
          "$120K - $180K + eligible for stock options",
          "Health, Dental, 401K, 20 days PTO"
        ]
      }
    },
    {
      "employer_name": "Motion Recruitment",
      "employer_logo": "https://www.motionrp.com/static/img/global/motion-recruitment-logo.png",
      "employer_website": "http://motionrecruitment.com",
      "job_publisher": "Dice",
      "job_id": "uW6uX_sH_e4AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Sr. Data Engineer (Python, Flink)",
      "job_apply_link": "https://www.dice.com/job-detail/d79b94a6-c6a0-45d3-904f-ca07eaea06af",
      "job_description": "This large eCommerce company is looking for a remote Sr. Data Engineer who is strong with Python, AWS, and Big Data (Hive, Flink, etc.). This is a full-time position that is remote\n\nThis eCommerce company generates the highest traffic worldwide, making it to the top 10 in competition with Amazon. They have established a presence in multiple states across the US and have received nothing but positive press after their most recent $100 billion dollar valuation. They need a Sr. Data Engineer who is an expert with Python, AWS, and Big Data.\nRequired Skills & Experience\n• 5+ years of experience\n• Python\n• AWS\n• Big Data with Hive, Spark, Flink\nDesired Skills & Experience\n• Can speak Mandarin fluently\n• Experience with Flink\n• Masters degree in Computer Science or related field\nWhat You Will Be Doing\nTech Breakdown\n• 100% Data Engineering\n\nDaily Responsibilities\n• 80% Hands On\n• 0% Management Duties\n• 20% Team Collaboration\nThe Offer\n• Bonus eligibility\n• Stock options\n\nYou will receive the following benefits:\n• Medical Insurance\n• Dental Benefits\n• Vision Benefits\n• Paid Time Off (PTO)\n• 401(k) {including match- if applicable}\n\nApplicants must be currently authorized to work in the US on a full-time basis now and in the future.",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675646535,
      "job_posted_at_datetime_utc": "2023-02-06T01:22:15.000Z",
      "job_city": "Irvine",
      "job_state": "CA",
      "job_country": "US",
      "job_latitude": 33.684566,
      "job_longitude": -117.82651,
      "job_benefits": [
        "paid_time_off",
        "retirement_savings",
        "health_insurance"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=0&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=uW6uX_sH_e4AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-09T01:22:15.000Z",
      "job_offer_expiration_timestamp": 1678324935,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "5+ years of experience",
          "Big Data with Hive, Spark, Flink",
          "Applicants must be currently authorized to work in the US on a full-time basis now and in the future"
        ],
        "Responsibilities": [
          "80% Hands On",
          "0% Management Duties"
        ],
        "Benefits": [
          "Bonus eligibility",
          "Stock options",
          "Medical Insurance",
          "Dental Benefits",
          "Vision Benefits",
          "Paid Time Off (PTO)",
          "401(k) {including match- if applicable}"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "WJTV Jobs",
      "job_id": "aXfrv0bIs_kAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.wjtv.com/jobs/staff-data-engineer-remote-usa-raleigh-north-carolina/906541135-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612677,
      "job_posted_at_datetime_utc": "2023-02-05T15:57:57.000Z",
      "job_city": "Raleigh",
      "job_state": "NC",
      "job_country": "US",
      "job_latitude": 35.77959,
      "job_longitude": -78.638176,
      "job_benefits": [
        "retirement_savings",
        "health_insurance",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=0&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=aXfrv0bIs_kAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Burnalong",
      "employer_logo": null,
      "employer_website": "http://www.burnalong.com",
      "job_publisher": "SimplyHired",
      "job_id": "OEVdrg4lOiUAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Backend (Python) and Data Engineer",
      "job_apply_link": "https://www.simplyhired.com/job/nDy3dPQixhxZKM_FSuowu5UvoolZ6kXwELaihcpgaL6UjcWZ7yLKfQ",
      "job_description": "BurnAlong is growing in many amazing ways and we could use your help!\n\nBurnalong is looking for Python Developers to join our engineering team and help us develop and maintain various software products and components/services.\n\nIf you like working in a fast-paced and collaborative environment and want to help people, BurnAlong is the place for you. It’s our mission to change lives by giving people the health and wellbeing programming and support they need. We are delivering truly innovative solutions that push the boundaries of tech while we transform an industry. Most importantly, we are here to support people and build a supportive and positive community; we are very proud to do so during these challenging times.\n\nWe are looking for a problem solver, not just a doer. Become part of a culture of integrity, collaboration, innovation, and creativity. We win as a team – we make big things happen by working together. Excellent oral and written communication is important both internally across the organization and externally with clients.\n\nJob Summary:\n\nPython Developer responsibilities include writing and testing code, debugging programs and integrating applications with third-party web services. To be successful in this role, you should have experience using server-side logic and work well in a team. Ultimately, you’ll build highly responsive web applications that align with our business needs.\n\nSkills\n\nStrong Fundamental Python Developer\n\nPython Web Development (Django, Flask)\n\nPython Automation Scripting (CI/CD and Monitoring)\n\nPython Automated Test Development (PyTest, Selenium, Robot Framework)\n\nWeb, Mobile & API Testing\n\nDatabase (SQL)\n\nData Analysis and Modeling\n\nAWS Cloud Experience\n\nFrontend Experience a Plus but Not Required\n\nExecute Principles Of Traceability From Requirements, Specs, Tests & Results\n\nGood Organization and Written Communication Status (e.g., Test Plans & Results)\n\nStructured Problem Solving and Root Cause Analysis (e.g., 5-Ys)\n\nQuality System Thinking or Experience (e.g. ISO 27001, 9001, Six Sigma) a Plus\n\nGood Communication, Planning/Organization & Teamwork Skills\n\n3+ Years Hands-On Experience\n\nAbout you:\n\nYou’re passionate about building a great product and you’re a quick learner who is\n\npassionate about your profession and role on the team. You take initiative to self-teach when needed. You have the ability to work independently and as part of a team with a strong attention to detail. You approach problems from a product perspective, thinking through how the user will interact with what we are building. You have strong communication skills. You ask questions, let others know when you need help, and tell others what you need;\n\nYou are a problem solver. You believe the best work is the result of finding the simplest\n\nsolution to complex challenges. You have a strong desire to learn and try new things!\n\nAbout BurnAlong:\n\nWhen friends want to play video games together but cannot do so in person, they play online\n\nTogether. When we cannot do an in-person meeting, we connect using tools like Zoom, Skype, FaceTime or Google Meet. When we cannot make it to the gym with friends, why should we be stuck at home working out with solo, one-directional experiences and instructors who we don’t know?\n\nBurnAlong comes to solve the problem of life often getting in the way of staying mentally and physically fit with your friends and instructors who you know and trust from your local gym, studio or wellness professional. We have 45+ categories to choose from and more\n\nthan 30,000 classes.\n\nWhole Mind, Whole Body, Whole Life\n\nWe’ve built BurnAlong to bridge the gap between online and in person, to help those who are struggling to keep wellness and fitness engaging and social. How? On BurnAlong you can take classes live online with the friends you invite and your favorite local wellness instructors. Less Hollywood and more neighborhood, authentic people helping each other stay connected and fit (mentally & physically). It’s a surprisingly cool experience and exploding.\n\nApply Now and Join Us.\n\nThis is a rare and exciting opportunity to join a startup that already has global momentum. We have lots of roles to fill and interesting projects to work on. We make a difference in people’;s lives and you can too! There is lots of room for growth with the company.\n\nIf you are passionate about what you do, you do it well, and are looking to be a part of an awesome movement, Burnalong is the right place for you. Check out our site for more",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675602502,
      "job_posted_at_datetime_utc": "2023-02-05T13:08:22.000Z",
      "job_city": null,
      "job_state": null,
      "job_country": "US",
      "job_latitude": 37.09024,
      "job_longitude": -95.71289,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=OEVdrg4lOiUAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 36,
        "experience_mentioned": true,
        "experience_preferred": true
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": false,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Strong Fundamental Python Developer",
          "AWS Cloud Experience",
          "Structured Problem Solving and Root Cause Analysis (e.g., 5-Ys)",
          "Good Communication, Planning/Organization & Teamwork Skills",
          "3+ Years Hands-On Experience",
          "You’re passionate about building a great product and you’re a quick learner who is",
          "passionate about your profession and role on the team",
          "You take initiative to self-teach when needed",
          "You have the ability to work independently and as part of a team with a strong attention to detail",
          "You approach problems from a product perspective, thinking through how the user will interact with what we are building",
          "You have strong communication skills",
          "You believe the best work is the result of finding the simplest",
          "You have a strong desire to learn and try new things!"
        ],
        "Responsibilities": [
          "Python Developer responsibilities include writing and testing code, debugging programs and integrating applications with third-party web services",
          "Python Automation Scripting (CI/CD and Monitoring)"
        ]
      }
    },
    {
      "employer_name": "System One",
      "employer_logo": "https://www.systemoneservices.com/wp-content/uploads/2016/12/System_One_Logo_WEB_300x300.jpg",
      "employer_website": "http://www.systemoneservices.com",
      "job_publisher": "LinkedIn",
      "job_id": "nRdiSky3JfQAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Cloud Data Engineer - Remote",
      "job_apply_link": "https://www.linkedin.com/jobs/view/cloud-data-engineer-remote-at-system-one-3471039075",
      "job_description": "Direct Hire - Fully Remote; No C2C\n\nUS Citizenship and the ability to obtain and maintain a Public Trust clearance\n\nALTA IT Services is looking for a Cloud Data Engineer to work remotely for a growing and exciting company. You will be responsible for developing and supporting the creation of the data fabric – ETLs, data movement, metadata management and data preparation working closely with the data platform engineer and other data & application stakeholders.\n\nWhat You Bring\n• Successful candidate will be able to rapidly support the assessment, planning and development of data pipelines, feeds and models along with other reusable components using AWS Glue, Athena, Aurora, S3, API gateway and Web service backends\n• Understand requirements and specifications such as transformation rules and logic, compute cost optimization and data SLAs and data reliability engineering needs\n• Utilize identified python and AWS tools and services specifically data-oriented python libraries and utilities such as pandas, great expectations and create reusable modular components\n• Create and execute medium to complex SQL scripts to validate and test data prior to and during the lifecycle of reports and dataset development\n• Ability to develop and maintain simple to mid-complexity reports/dashboards in tools like Tableau and AWS Quick sight\n\nRequired Skills\n• Typically requires a bachelor's degree in Computer Science, Information Systems, engineering, business, or other related scientific or technical discipline. Years of experience may be considered as a substitution.\n• Successful track record in handling complex datasets with minimal supervision\n• Experience handling and converting JSON objects in a relational database\n• Ability to support a variety of structured and semi-structured data in streaming and batch frameworks\n• Experience with troubleshooting, monitoring and coordinating defect resolution related to all dashboard components including access and performance\n• Creating and supporting data visualizations, queries, components and modules within the current scope of the system\n• 5+ years of hands-on experience with supporting and enabling complex dashboards and SQL and NoSQL backends\n• 3+ year of work experience on AWS based data sources such as RDS, Athena, Redshift, Lambda, Postgres\n• AWS certifications\n• 3+ years of work experience with Terraform\n• 1+ year of experience using complex data sets and formats such as JSON, AVRO, parquet\n• 1+ year of experience in data profiling and quality management\n• 1+ year experience working with CI/CD tools including Git for ETL and scripts repository\n\nDesired Skills\n• Experience with big data tools such as EMR/Spark, Databricks/PySpark is an advantage\n• 3+ years of work experience with Node.js\n• Experience working with database versioning tools such as Flyway is an advantage\n• Experience in Jenkins scripting is an advantage\n• Education: Bachelor's degree in Computer Science or related discipline\n\nSystem One, and its subsidiaries including Joulé, ALTA IT Services, CM Access, and MOUNTAIN, LTD., are leaders in delivering outsourced services and workforce solutions across North America. We help clients get work done more efficiently and economically, without compromising quality. System One not only serves as a valued partner for our clients, but we offer eligible employees health and welfare benefits coverage options including medical, dental, vision, spending accounts, life insurance, voluntary plans, as well as participation in a 401(k) plan.\n\nSystem One is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, age, national origin, disability, family care or medical leave status, genetic information, veteran status, marital status, or any other characteristic protected by applicable federal, state, or local law.",
      "job_is_remote": true,
      "job_posted_at_timestamp": 1675638002,
      "job_posted_at_datetime_utc": "2023-02-05T23:00:02.000Z",
      "job_city": "Herndon",
      "job_state": "VA",
      "job_country": "US",
      "job_latitude": 38.969555,
      "job_longitude": -77.3861,
      "job_benefits": [
        "health_insurance",
        "retirement_savings",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=nRdiSky3JfQAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T23:00:02.000Z",
      "job_offer_expiration_timestamp": 1678230002,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": true,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "US Citizenship and the ability to obtain and maintain a Public Trust clearance",
          "Create and execute medium to complex SQL scripts to validate and test data prior to and during the lifecycle of reports and dataset development",
          "Typically requires a bachelor's degree in Computer Science, Information Systems, engineering, business, or other related scientific or technical discipline",
          "Years of experience may be considered as a substitution",
          "Successful track record in handling complex datasets with minimal supervision",
          "Experience handling and converting JSON objects in a relational database",
          "5+ years of hands-on experience with supporting and enabling complex dashboards and SQL and NoSQL backends",
          "3+ year of work experience on AWS based data sources such as RDS, Athena, Redshift, Lambda, Postgres",
          "AWS certifications",
          "1+ year of experience using complex data sets and formats such as JSON, AVRO, parquet",
          "1+ year of experience in data profiling and quality management",
          "1+ year experience working with CI/CD tools including Git for ETL and scripts repository"
        ],
        "Responsibilities": [
          "You will be responsible for developing and supporting the creation of the data fabric – ETLs, data movement, metadata management and data preparation working closely with the data platform engineer and other data & application stakeholders",
          "Experience with troubleshooting, monitoring and coordinating defect resolution related to all dashboard components including access and performance"
        ],
        "Benefits": [
          "System One not only serves as a valued partner for our clients, but we offer eligible employees health and welfare benefits coverage options including medical, dental, vision, spending accounts, life insurance, voluntary plans, as well as participation in a 401(k) plan"
        ]
      }
    },
    {
      "employer_name": "BroadBean",
      "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/0/05/Broadbean-withstrapinside-fullcolour-rgb.png",
      "employer_website": "http://www.broadbean.com",
      "job_publisher": "Recruit.net",
      "job_id": "uUHvF-MIbecAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Senior Data Engineer",
      "job_apply_link": "https://www.recruit.net/job/data-engineer-jobs/AAF69D735F7E0736",
      "job_description": "Job Description\nJOB SUMMARY\nThe Senior Data Engineer is an interdisciplinary individual on the Data Analytics team who works closely with multiple stakeholders, across the enterprise and Information Technology (IT), to ensure that the most important data is accessible and well-understood.\nAs a Senior Data Engineer, you will be responsible for designing, developing, implementing, and supporting new and existing highly efficient ELT/ETL processes. You will work closely with data consumers, solution architects, security and governance teams to implement solutions to answer complex questions and drive business decisions. You will apply proven communication skills, problem-solving skills, and knowledge of best practices related to designing, developing, and deploying data and analytic solutions.\nSenior Data Engineers need to be adept in several technical and business skills. These include working with diverse datasets, parsing and understanding data, working with domain experts, data scientists and analysts in framing the business problem and provisioning integrated data quickly across multiple environments. Senior Data Engineers need to be inquisitive and motivated to learn new technologies and capabilities which could benefit the organization, and lead the effort in evaluating the technology for acceptance at TTX. Senior Data Engineers will also assist the business data science efforts including source data, build data sets, help evaluate models and integrate analytics and data science model outputs into business processes.\nRESPONSIBILITIES\n• Hybrid cloud environment: the Senior Data Engineer will work in a hybrid cloud ecosystem, building and supporting data and analytics solutions hosted in on-prem data centers and multiple cloud environments. The Senior Data Engineer will need to learn the data, tools and capabilities resident in each area of the environment.\n• Build data pipelines: Managed data pipelines consist of a series of stages through which data flows. These data pipelines have to be created, maintained and optimized as workloads move from development to production. Designing, build and maintaining data pipelines, in the hybrid cloud, will be the primary responsibility of the data engineer.\n• Drive data centric decision making. The Senior Data Engineer will assist with enhancing the data and metadata management infrastructure to ensure data quality, accessibility and security.\n• Collaborate across departments: The Senior Data Engineer will need strong collaboration skills in order to work with varied stakeholders within the organization. In particular, the Senior Data Engineer will work in close relationship with business data consumers, of various skill levels, in refining their data requirements for various data and analytics initiatives.\n• Lead, educate and train: The Senior Data Engineer should be curious and knowledgeable about new technologies and data initiatives. They will propose data ingestion, preparation, integration and operationalization tools or techniques in addressing these requirements. The Senior Data Engineer will train team members, data consumers, data scientists and data analysts in these technologies and preparation techniques.\n• Participate in ensuring compliance and governance during data use: It will be the responsibility of the Senior Data Engineer to ensure that the data users and consumers consume the data provisioned to them responsibly through data governance and compliance initiatives. Data engineers should work with data governance teams (and information stewards within these teams) and participate in vetting and promoting content created in the business and by data scientists to the curated data catalog for governed reuse.\n• Become a data and analytics evangelist: The Senior Data Engineer will be considered a blend of data and analytics \"evangelist,\" \"guru\" and \"fixer.\" This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals.\nQUALIFICATIONS\nEducation and Training\n• A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is required.\n• An advanced degree in computer science (MS), statistics, applied mathematics (Ph.D.), information science (MIS), data management, information systems, information science (post-graduation diploma or related) or a related quantitative field [or equivalent work experience] is preferred.\n• The ideal candidate will have a combination of IT skills, data governance skills, analytics skills, and communication skills.\nPrevious Experience\n• At least 5 years or more of work experience in data management disciplines including [data integration, modeling, optimization and data quality], and/or other areas directly relevant to data engineering responsibilities and tasks.\n• At least 5 years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative.\nTechnical [and Business] Knowledge/Skills\n• Strong experience with various languages and advanced analytics tools such as SQL, Python, R, PowerBI, Microsoft SSIS/SSAS, Azure Synapse and others.\n• Strong ability to design, build and manage data structures and pipelines for encompassing data transformation, data models, schemas, metadata and workload management. Work with both IT and business in integrating analytics and data science output into business processes and workflows.\n• Strong experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using data integration technologies, including ETL/ELT, data replication/CDC, message-oriented data movement, API design and development.\n• Ability in the writing of reports, business correspondence, and procedure manuals. Effectively present information and respond to questions from groups of managers, clients, customers, and the general public.\n• Ability to define problems, collect data, establish facts, and draw valid conclusions. Ability to interpret an extensive variety of technical instructions in mathematical or diagram form and deal with several abstract and concrete variables.\n• Ability to work with mathematical concepts such as probability and statistical inference, and fundamentals of plane and solid geometry and trigonometry. Ability to apply concepts such as fractions, percentages, ratios, and proportions to practical situations.\nPHYSICAL JOB REQUIREMENTS\nThe physical demands and work environment characteristics described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable qualified individuals with disabilities to perform the essential functions.\nGeneral office environment with limited physical demands and travel. While performing the duties of this job, the employee is regularly required to communicate using hands and fingers, handle, feel, and reach. The employee frequently is required to remain stationary within work area and occasionally move about. The employee must occasionally transport up to 10 pounds. Correctable vision and hearing required.\nThe above job description is intended to describe the general content of and requirements for the performance of this job. It is not to be construed as an exhaustive statement of duties, responsibilities or requirements.\nAbout Us\nTTX Company is a leading provider of railcars and related freight car management services to the North American rail industry. TTX's pool of railcars is ideal for supporting shippers in the intermodal, automotive, paper & forest, metals, machinery, wind energy and other markets where flatcars, boxcars and gondolas are required.\nTTX Company is an Equal Employment Opportunity Employer.",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675641600,
      "job_posted_at_datetime_utc": "2023-02-06T00:00:00.000Z",
      "job_city": null,
      "job_state": "IL",
      "job_country": "US",
      "job_latitude": 40.633125,
      "job_longitude": -89.39853,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=uUHvF-MIbecAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-02-11T00:00:00.000Z",
      "job_offer_expiration_timestamp": 1676073600,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": true
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Senior Data Engineers need to be adept in several technical and business skills",
          "Senior Data Engineers need to be inquisitive and motivated to learn new technologies and capabilities which could benefit the organization, and lead the effort in evaluating the technology for acceptance at TTX",
          "Collaborate across departments: The Senior Data Engineer will need strong collaboration skills in order to work with varied stakeholders within the organization",
          "A bachelor's in computer science, statistics, applied mathematics, data management, information systems, information science or a related quantitative field [or equivalent work experience] is required",
          "The ideal candidate will have a combination of IT skills, data governance skills, analytics skills, and communication skills",
          "At least 5 years or more of work experience in data management disciplines including [data integration, modeling, optimization and data quality], and/or other areas directly relevant to data engineering responsibilities and tasks",
          "At least 5 years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiative",
          "Technical [and Business] Knowledge/Skills",
          "Strong experience with various languages and advanced analytics tools such as SQL, Python, R, PowerBI, Microsoft SSIS/SSAS, Azure Synapse and others",
          "Strong ability to design, build and manage data structures and pipelines for encompassing data transformation, data models, schemas, metadata and workload management",
          "Ability to interpret an extensive variety of technical instructions in mathematical or diagram form and deal with several abstract and concrete variables",
          "Ability to work with mathematical concepts such as probability and statistical inference, and fundamentals of plane and solid geometry and trigonometry",
          "Ability to apply concepts such as fractions, percentages, ratios, and proportions to practical situations",
          "The physical demands and work environment characteristics described here are representative of those that must be met by an employee to successfully perform the essential functions of this job",
          "Correctable vision and hearing required"
        ],
        "Responsibilities": [
          "As a Senior Data Engineer, you will be responsible for designing, developing, implementing, and supporting new and existing highly efficient ELT/ETL processes",
          "You will work closely with data consumers, solution architects, security and governance teams to implement solutions to answer complex questions and drive business decisions",
          "Senior Data Engineers will also assist the business data science efforts including source data, build data sets, help evaluate models and integrate analytics and data science model outputs into business processes",
          "Hybrid cloud environment: the Senior Data Engineer will work in a hybrid cloud ecosystem, building and supporting data and analytics solutions hosted in on-prem data centers and multiple cloud environments",
          "The Senior Data Engineer will need to learn the data, tools and capabilities resident in each area of the environment",
          "Build data pipelines: Managed data pipelines consist of a series of stages through which data flows",
          "These data pipelines have to be created, maintained and optimized as workloads move from development to production",
          "Designing, build and maintaining data pipelines, in the hybrid cloud, will be the primary responsibility of the data engineer",
          "Drive data centric decision making",
          "The Senior Data Engineer will assist with enhancing the data and metadata management infrastructure to ensure data quality, accessibility and security",
          "In particular, the Senior Data Engineer will work in close relationship with business data consumers, of various skill levels, in refining their data requirements for various data and analytics initiatives",
          "Lead, educate and train: The Senior Data Engineer should be curious and knowledgeable about new technologies and data initiatives",
          "They will propose data ingestion, preparation, integration and operationalization tools or techniques in addressing these requirements",
          "The Senior Data Engineer will train team members, data consumers, data scientists and data analysts in these technologies and preparation techniques",
          "Participate in ensuring compliance and governance during data use: It will be the responsibility of the Senior Data Engineer to ensure that the data users and consumers consume the data provisioned to them responsibly through data governance and compliance initiatives",
          "Data engineers should work with data governance teams (and information stewards within these teams) and participate in vetting and promoting content created in the business and by data scientists to the curated data catalog for governed reuse",
          "Become a data and analytics evangelist: The Senior Data Engineer will be considered a blend of data and analytics \"evangelist,\" \"guru\" and \"fixer.\" This role will promote the available data and analytics capabilities and expertise to business unit leaders and educate them in leveraging these capabilities in achieving their business goals",
          "Ability in the writing of reports, business correspondence, and procedure manuals",
          "Ability to define problems, collect data, establish facts, and draw valid conclusions",
          "While performing the duties of this job, the employee is regularly required to communicate using hands and fingers, handle, feel, and reach",
          "The employee frequently is required to remain stationary within work area and occasionally move about"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "WREG Jobs",
      "job_id": "kBxQ6mj_Yd0AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.wreg.com/jobs/staff-data-engineer-remote-usa-boston-massachusetts/906522982-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612410,
      "job_posted_at_datetime_utc": "2023-02-05T15:53:30.000Z",
      "job_city": "Boston",
      "job_state": "MA",
      "job_country": "US",
      "job_latitude": 42.36008,
      "job_longitude": -71.05888,
      "job_benefits": [
        "retirement_savings",
        "dental_coverage",
        "health_insurance"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=kBxQ6mj_Yd0AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Galderma",
      "employer_logo": "https://cdn.cookielaw.org/logos/9eb64978-23c0-4924-97a4-03eb6aab1106/83dc1859-114b-46cf-b32e-e3bbebc77feb/ad633653-014e-4c29-b120-efef9829e7a2/GALDERMA_LOGO_BLACK_RGB.jpg",
      "employer_website": "http://www.galderma.com",
      "job_publisher": "LinkedIn",
      "job_id": "HsN3QYRDCIwAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer",
      "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-galderma-3470918920",
      "job_description": "Data Engineer\n\nDallas, TX\n\nJob Summary\n\nThe Data Engineer leads initiatives to develop and implement business-driven BI and Analytics solutions to enable business to win in the markets and gain competitive advantage. This individual serves as the primary liaison between IT Analytics, Shared Services, and the Business Units.\n\nKey Responsibilities\n• Understand customers' overall data estate business, related success measures, and IT priorities in order to design data solutions that drive business value\n• Educate the business and promote best practices on how to leverage BI and analytics solutions, as well as help facilitate discussions to anticipate future needs and opportunities\n• Assist in the identification and integration of data sources\n• Clearly communicate findings, recommendations, and opportunities to improve data systems and solutions\n• Seek out information to learn about emerging methodologies and technologies\n• Clarify problems by driving to understand the true issue\n• Work closely with DevOps team to automate the deployment of resources to our various Azure subscription\n• Performs technical design reviews and code reviews\n• Other duties as assigned\n\nSkills & Qualifications\n• Bachelor's degree in Information Technology or a related field, required\n• 5 years of experience in implementing DWH / BI solutions\n• 3 years of experience leading projects in analysis, architecture, design, and development of traditional data warehouse, data pipeline and business intelligence solutions\n• 3 years of experience as a Data Engineer using Data Brick and Snowflake\n• Good understanding of various Data Models such as Dimensional Data Modeling and DataVault Modeling\n• Experience with Azure cloud services such as azure cloud security, monitoring, logging, scaling, caching, etc.\n• Experience dealing with cloud workloads, application, and infrastructure architecture, data ingestion architecture, data storage, and transformations, data analytics, serverless function/application design, micro-services, DevOps\n• Experience with Linux and cloud environment including commands and scripting\n• Solid experience in writing complex SQL queries as well as PL/SQL programs on Oracle database\n• Familiarity with DevOps best practices and automation of documentation, testing, build, deployment, configuration, and monitoring\n• Experience in visualization tools/solution such as Tableau and Power BI\n• Ability to work independently and collaboratively working in a fast-paced environment\n• Solid communication skills, particularly in writing technical solution proposals\n\nWhat we offer\n\nYou will be working for an organization that embraces diversity & inclusion and believe we will deliver better outcomes by reflecting the perspectives of our diverse customer base. You will also have access to a range of company benefits, including a competitive wage with shift differential, annual bonus opportunities and career advancement and cross-training.\n\nEmployer's Rights:\n\nThis job description does not list all the duties of the job. You may be asked by your supervisors or managers to perform other duties. You will be evaluated in part based upon your performance of the tasks listed in this job description. The employer has the right to revise this job description at any time. This job description is not a contract for employment, and either you or the employer may terminate employment at any time, for any reason. In addition, reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position.",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675627647,
      "job_posted_at_datetime_utc": "2023-02-05T20:07:27.000Z",
      "job_city": "Dallas",
      "job_state": "TX",
      "job_country": "US",
      "job_latitude": 32.776665,
      "job_longitude": -96.79699,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=HsN3QYRDCIwAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T20:07:26.000Z",
      "job_offer_expiration_timestamp": 1678219646,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": true,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Bachelor's degree in Information Technology or a related field, required",
          "5 years of experience in implementing DWH / BI solutions",
          "3 years of experience leading projects in analysis, architecture, design, and development of traditional data warehouse, data pipeline and business intelligence solutions",
          "3 years of experience as a Data Engineer using Data Brick and Snowflake",
          "Good understanding of various Data Models such as Dimensional Data Modeling and DataVault Modeling",
          "Experience with Azure cloud services such as azure cloud security, monitoring, logging, scaling, caching, etc",
          "Experience dealing with cloud workloads, application, and infrastructure architecture, data ingestion architecture, data storage, and transformations, data analytics, serverless function/application design, micro-services, DevOps",
          "Experience with Linux and cloud environment including commands and scripting",
          "Solid experience in writing complex SQL queries as well as PL/SQL programs on Oracle database",
          "Familiarity with DevOps best practices and automation of documentation, testing, build, deployment, configuration, and monitoring",
          "Experience in visualization tools/solution such as Tableau and Power BI",
          "Ability to work independently and collaboratively working in a fast-paced environment",
          "Solid communication skills, particularly in writing technical solution proposals"
        ],
        "Responsibilities": [
          "Understand customers' overall data estate business, related success measures, and IT priorities in order to design data solutions that drive business value",
          "Educate the business and promote best practices on how to leverage BI and analytics solutions, as well as help facilitate discussions to anticipate future needs and opportunities",
          "Assist in the identification and integration of data sources",
          "Clearly communicate findings, recommendations, and opportunities to improve data systems and solutions",
          "Seek out information to learn about emerging methodologies and technologies",
          "Clarify problems by driving to understand the true issue",
          "Work closely with DevOps team to automate the deployment of resources to our various Azure subscription",
          "Performs technical design reviews and code reviews",
          "Other duties as assigned"
        ],
        "Benefits": [
          "You will also have access to a range of company benefits, including a competitive wage with shift differential, annual bonus opportunities and career advancement and cross-training"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "My ArkLaMiss Jobs",
      "job_id": "GWFD-CmV_EoAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.myarklamiss.com/jobs/staff-data-engineer-remote-usa-fort-worth-texas/906534885-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612583,
      "job_posted_at_datetime_utc": "2023-02-05T15:56:23.000Z",
      "job_city": "Fort Worth",
      "job_state": "TX",
      "job_country": "US",
      "job_latitude": 32.75549,
      "job_longitude": -97.330765,
      "job_benefits": [
        "dental_coverage",
        "retirement_savings",
        "health_insurance"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=GWFD-CmV_EoAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Compunnel Inc.",
      "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSVPlO54B9OYn70g3qLrZv9sQPsKHS9lJXF8hEF&s=0",
      "employer_website": "http://www.compunnel.com",
      "job_publisher": "LinkedIn",
      "job_id": "5pDBhRAFPpoAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer(Compunnel's W2\"NO C2C\"",
      "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-compunnel-s-w2-no-c2c-at-compunnel-inc-3471108163",
      "job_description": "No C2C\n\nPosition: Senior Data Engineer\n\nPrimary Skills : Azure ,SQL\n\nShould have 7+ years of experience.\n\nJOB DESCRIPTION:\n\nThe Senior Data Engineer is a core member of the team responsible for providing digital solutions to a range of business challenges. This role will be responsible for developing, optimizing, and standardizing business processes using the full suite of Microsoft tools and adding value to the data architecture. Perform data model design, data formatting, and ETL development. In addition to designing and providing solutions, they are at times responsible for developing, incorporating, training, testing, and supporting the solution. Personal traits require is the ability to speak to customers about the program, gather future requirements, and work cross functional with other teams. This position is part of the strategy for the company going forward in terms of research, usability, functionality, cost, and system structure.\n\nPOSITION RESPONSIBILITIES:\n• Follow all company safety polices. Be proactive and diligent in identifying any safety issues.\n• Work with IT and Business management to evaluate complex user requests, projects, and clarify objectives.\n• Analysis feasibility, cost, and time required related to business request. Insure compatibility with current system and system capabilities.\n• Take on project responsibilities and prepare supporting project documentation as needed.\n• Work closely with functional architects to define and implement application architecture.\n• Develop new code or modify computer programs into existing systems. Must protect system landscapes by preforming testing following IT policies.\n• Effectively manage level one and level two business support. This will include evaluating error messages, modify codes, and troubleshoot programming problems. All support must meet defined service level agreements.\n• Perform analysis of systems issues or changes. Effectively operate under current change management process.\n• Prepare training documentation of new or existing programs.\n• Prepare an implementation plan of the new software solutions being delivered to the business.\n• Will be responsible for researching technologies and trends.\n• Participate in design, implementation, and support of a data warehouse and analytics platform utilizing Azure cloud technologies.\n• Hands-on experience implementing data processing using Azure services: Azure Data Factory, Azure Synapse Analytics, Azure Event Hub, Azure Functions, Azure Analysis Service & Azure Databricks.\n• Design, develop, and support Azure SQL Database Data Marts for functional area data consumers.\n• Exposure to the latest cloud-based data warehouse technologies\n• Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using ADF.\n• Collaborate with DW developers for products that require reporting data and ensure that datasets are in place and are used consistently internally/externally.\n• Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.\n\nREQUIRED SKILLS AND EXPERIENCE:\n• Bachelor's degree in Computer Science, Information Technology, or related field.\n• Self-motivated and driven individual who has a strong technical understanding across a broad range of technologies and solutions.\n• Strong technical and analytical skills\n• Innovative thinker who is positive, proactive, and embraces change.\n• Must have the aptitude to learn new technologies through self-study and trial and error.\n• Ability to understand business processes and willingness to see them in action.\n• 5+ years of experience as Data Engineer with strong SQL skills\n• 2+ years of experience in Azure data platform - Azure data factory, Azure Data bricks, Power BI, Azure Synapse\n• 3+ Years of experience with data and application integrations using logic apps or leveraging API’s using REST and SOAP. Well versed with XML, JSON and data manipulation\n• Experience in application integration projects with exposure to Microsoft Azure Services (App Service, Azure Service bus, API Management, Logic Apps and Azure Functions) is a plus.\n• Experience building data models and performing complex queries.\n• Hands-on experience building data pipelines and developing data quality and data monitoring programs.",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675651184,
      "job_posted_at_datetime_utc": "2023-02-06T02:39:44.000Z",
      "job_city": null,
      "job_state": null,
      "job_country": "US",
      "job_latitude": 37.09024,
      "job_longitude": -95.71289,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=5pDBhRAFPpoAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-08T02:39:44.000Z",
      "job_offer_expiration_timestamp": 1678243184,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 84,
        "experience_mentioned": true,
        "experience_preferred": true
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": true,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Primary Skills : Azure ,SQL",
          "Should have 7+ years of experience",
          "Bachelor's degree in Computer Science, Information Technology, or related field",
          "Self-motivated and driven individual who has a strong technical understanding across a broad range of technologies and solutions",
          "Strong technical and analytical skills",
          "Innovative thinker who is positive, proactive, and embraces change",
          "Must have the aptitude to learn new technologies through self-study and trial and error",
          "Ability to understand business processes and willingness to see them in action",
          "5+ years of experience as Data Engineer with strong SQL skills",
          "2+ years of experience in Azure data platform - Azure data factory, Azure Data bricks, Power BI, Azure Synapse",
          "3+ Years of experience with data and application integrations using logic apps or leveraging API’s using REST and SOAP",
          "Well versed with XML, JSON and data manipulation",
          "Experience building data models and performing complex queries",
          "Hands-on experience building data pipelines and developing data quality and data monitoring programs"
        ],
        "Responsibilities": [
          "The Senior Data Engineer is a core member of the team responsible for providing digital solutions to a range of business challenges",
          "This role will be responsible for developing, optimizing, and standardizing business processes using the full suite of Microsoft tools and adding value to the data architecture",
          "Perform data model design, data formatting, and ETL development",
          "In addition to designing and providing solutions, they are at times responsible for developing, incorporating, training, testing, and supporting the solution",
          "Personal traits require is the ability to speak to customers about the program, gather future requirements, and work cross functional with other teams",
          "Follow all company safety polices",
          "Be proactive and diligent in identifying any safety issues",
          "Work with IT and Business management to evaluate complex user requests, projects, and clarify objectives",
          "Analysis feasibility, cost, and time required related to business request",
          "Insure compatibility with current system and system capabilities",
          "Take on project responsibilities and prepare supporting project documentation as needed",
          "Work closely with functional architects to define and implement application architecture",
          "Develop new code or modify computer programs into existing systems",
          "Must protect system landscapes by preforming testing following IT policies",
          "Effectively manage level one and level two business support",
          "This will include evaluating error messages, modify codes, and troubleshoot programming problems",
          "All support must meet defined service level agreements",
          "Perform analysis of systems issues or changes",
          "Effectively operate under current change management process",
          "Prepare training documentation of new or existing programs",
          "Prepare an implementation plan of the new software solutions being delivered to the business",
          "Will be responsible for researching technologies and trends",
          "Participate in design, implementation, and support of a data warehouse and analytics platform utilizing Azure cloud technologies",
          "Hands-on experience implementing data processing using Azure services: Azure Data Factory, Azure Synapse Analytics, Azure Event Hub, Azure Functions, Azure Analysis Service & Azure Databricks",
          "Design, develop, and support Azure SQL Database Data Marts for functional area data consumers",
          "Exposure to the latest cloud-based data warehouse technologies",
          "Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using ADF",
          "Collaborate with DW developers for products that require reporting data and ensure that datasets are in place and are used consistently internally/externally",
          "Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "WJHL Jobs",
      "job_id": "MitaAp12HtUAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.wjhl.com/jobs/staff-data-engineer-remote-usa-cleveland-ohio/906546161-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612763,
      "job_posted_at_datetime_utc": "2023-02-05T15:59:23.000Z",
      "job_city": "Cleveland",
      "job_state": "OH",
      "job_country": "US",
      "job_latitude": 41.49932,
      "job_longitude": -81.69436,
      "job_benefits": [
        "retirement_savings",
        "health_insurance",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=MitaAp12HtUAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "KHON2 Jobs",
      "job_id": "a8_WC4Dj7sAAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.khon2.com/jobs/staff-data-engineer-remote-usa-washington-washington-dc/906521974-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612395,
      "job_posted_at_datetime_utc": "2023-02-05T15:53:15.000Z",
      "job_city": "Washington",
      "job_state": "DC",
      "job_country": "US",
      "job_latitude": 38.907192,
      "job_longitude": -77.03687,
      "job_benefits": [
        "health_insurance",
        "dental_coverage",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=a8_WC4Dj7sAAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Iron EagleX, Inc",
      "employer_logo": "https://media.licdn.com/dms/image/C560BAQHkTDqrowqGIw/company-logo_200_200/0/1547680415037?e=2147483647&v=beta&t=ot-6CzBoHsff-8han_l7nAmftSbrg0w7RhOTWh6cwBA",
      "employer_website": "http://ironeaglex.com",
      "job_publisher": "Clearance Jobs",
      "job_id": "r0Tlhdr_VakAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/6988883/data-engineer",
      "job_description": "Overview\n\nIron EagleX is a veteran owned defense contracting company based in Tampa, FL.\n\nIt is our mission to provide solutions to the most challenging technical problems facing the Department of Defense while simultaneously making a positive impact on our employees and community.\n\nResponsibilities\n\nJob Description:\n\nThe data engineer has a deep understanding of performance optimization and data pipelining. In addition to the baseline skills of a data analyst, data engineers can make raw data more useful for the enterprise.\n\nResponsibilities:\n• Data engineers can create and integrate application programming interfaces (APIs). Their technical skills generally include multiple programming languages and a deep knowledge of SQL database design.\n• The data engineer role requires a more in-depth knowledge in programming for integrating complex models and using advanced software library frameworks to distribute large, clustered data sets. Data engineers collect and arrange data in a form that is useful for analytics.\n• A basic knowledge in machine learning is also required to build efficient and accurate data pipelines to meet the needs for downstream users such as data scientists to create the models and analytics that produce insight.\n\nJob Duties Include (but not limited to):\n• Developing, maintaining, and testing infrastructures for data generation to transform data from various structured and unstructured data sources.\n• Develop complex queries to ensure accessibility while optimizing the performance of NoSQL and or big data infrastructure. Create and maintain optimal data pipeline architecture.\n• Build and maintain the infrastructure to support extraction, transformation, and loading (ETL) of data from a wide variety of data sources. Extract data from multiple data sources, relational SQL and NoSQL databases, and other platform APIs, for data ingestion and integration.\n• Configure and manage data analytic frameworks and pipelines using databases and tools such as NoSQL, SQL, HDInsight, MongoDB, Cassandra, Neo4j, GraphDB, OrientDB, Spark, Hadoop, Kafka, Hive, and Pig.\n• Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms.\n• Administrate cloud computing and CI/CD pipelines to include Azure, Google, and Amazon Web Service (AWS).\n• Coordinate with stakeholders, including product, data and design teams to assist with data-related technical issues and support their data infrastructure needs\n\nQualifications\n\nRequired Skills & Experience:\n• Minimum of 1-year experience is required\n• Verifiable work experience working with data structures, database management, distributed computing, and API driven architectures using SQL and No-SQL engines.\n• Proficient in modeling frameworks like Universal Modeling Language (UML), Agile Development, and Git Operations.\n• MUST BE A U.S. CITIZEN\n\nEducation & Certifications:\n• Bachelor's degree in a STEM field with preference towards Computer Science and Software Engineering.\n• A Certified Data Management Professional certification is preferred.\n\nSecurity Clearance:\n• At least Interim Top Secret clearance is required, while Top Secret clearance eligible for SCI is preferred.\n\nBenefits:\n• National health, vision, and dental plans\n• 20 days of PTO and 11 paid holidays\n• Life Insurance\n• Short and long term disability plans\n• 401(K) retirement plan\n• Incentive and recognition programs\n• Relocation opportunities\n\nIron EagleX is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, among other things, or status as a qualified individual with disability.",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675617312,
      "job_posted_at_datetime_utc": "2023-02-05T17:15:12.000Z",
      "job_city": "Fayetteville",
      "job_state": "NC",
      "job_country": "US",
      "job_latitude": 35.052666,
      "job_longitude": -78.87836,
      "job_benefits": [
        "dental_coverage",
        "health_insurance",
        "paid_time_off",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=10&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=r0Tlhdr_VakAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:12.000Z",
      "job_offer_expiration_timestamp": 1680739212,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 12,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Their technical skills generally include multiple programming languages and a deep knowledge of SQL database design",
          "Minimum of 1-year experience is required",
          "Verifiable work experience working with data structures, database management, distributed computing, and API driven architectures using SQL and No-SQL engines",
          "Proficient in modeling frameworks like Universal Modeling Language (UML), Agile Development, and Git Operations",
          "MUST BE A U.S",
          "Bachelor's degree in a STEM field with preference towards Computer Science and Software Engineering"
        ],
        "Responsibilities": [
          "Data engineers can create and integrate application programming interfaces (APIs)",
          "The data engineer role requires a more in-depth knowledge in programming for integrating complex models and using advanced software library frameworks to distribute large, clustered data sets",
          "Data engineers collect and arrange data in a form that is useful for analytics",
          "A basic knowledge in machine learning is also required to build efficient and accurate data pipelines to meet the needs for downstream users such as data scientists to create the models and analytics that produce insight",
          "Developing, maintaining, and testing infrastructures for data generation to transform data from various structured and unstructured data sources",
          "Develop complex queries to ensure accessibility while optimizing the performance of NoSQL and or big data infrastructure",
          "Create and maintain optimal data pipeline architecture",
          "Build and maintain the infrastructure to support extraction, transformation, and loading (ETL) of data from a wide variety of data sources",
          "Extract data from multiple data sources, relational SQL and NoSQL databases, and other platform APIs, for data ingestion and integration",
          "Configure and manage data analytic frameworks and pipelines using databases and tools such as NoSQL, SQL, HDInsight, MongoDB, Cassandra, Neo4j, GraphDB, OrientDB, Spark, Hadoop, Kafka, Hive, and Pig",
          "Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms",
          "Administrate cloud computing and CI/CD pipelines to include Azure, Google, and Amazon Web Service (AWS)",
          "Coordinate with stakeholders, including product, data and design teams to assist with data-related technical issues and support their data infrastructure needs"
        ],
        "Benefits": [
          "National health, vision, and dental plans",
          "20 days of PTO and 11 paid holidays",
          "Life Insurance",
          "Short and long term disability plans",
          "401(K) retirement plan",
          "Incentive and recognition programs",
          "Relocation opportunities"
        ]
      }
    },
    {
      "employer_name": "System One",
      "employer_logo": "https://www.systemoneservices.com/wp-content/uploads/2016/12/System_One_Logo_WEB_300x300.jpg",
      "employer_website": "http://www.systemoneservices.com",
      "job_publisher": "LinkedIn",
      "job_id": "-eyDleC8PhgAAAAAAAAAAA==",
      "job_employment_type": "CONTRACTOR",
      "job_title": "Data Engineer (Cloud Analytics)",
      "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-cloud-analytics-at-system-one-3471038136",
      "job_description": "Location: REMOTE with a possibility of a Hybrid Work Model (Vienna, VA)\n\nPay Rate: Open to Both C2C and W2 options\n\nPosition Type: Multiyear Contract\n\nDescription\n\nThis Data Engineer (Cloud Analytics) will be support data sourcing strategies, designs, and implementation for the Client Enterprise Cloud Data Lake by designing, building, and integrating data from various sources in support of predictive and adaptive analytics.\n\nRequired (Individual Role)\n• Bachelor’s degree in Information Systems, Computer Science, Engineering, or related field\n• Hands-on experience creating automated data pipelines using modern technology stacks for batch ETL, data streaming, or change data capture, and for data processing to load advanced analytics data repositories\n• Proficient in designing and implementing data integration processes in a large, distributed environment using cloud services e.g., Azure Event Hub, Data Factory, Data Catalog, Databricks, Stream Analytics, Apache Spark\n• Experience in designing data lake storage structures, data acquisition, transformation, and distribution processing\n• Proficient in developing and operationalizing various data distribution patterns such as APIs, event based, publish/subscribe models\n• Experience migrating data lakes from on-premises to cloud\n• Experience using the Apache Hadoop technology stack\n• Advanced experience in SQL programming\n• Proficient in programming languages (e.g., Python, Java)\n• Experience with agile delivery using Jira, or Azure DevOps\n• Experience monitoring performance of data workflows and infrastructure through use of monitoring tools and enabling alerts\n• Functional knowledge of data visualization tools\n• Excellent written and oral communication skills\n\nFor immediate consideration, please apply directly or contact Ryan Pustilnik at 301.740.2110\n\nSystem One, and its subsidiaries including Joulé, ALTA IT Services, CM Access, and MOUNTAIN, LTD., are leaders in delivering outsourced services and workforce solutions across North America. We help clients get work done more efficiently and economically, without compromising quality. System One not only serves as a valued partner for our clients, but we offer eligible employees health and welfare benefits coverage options including medical, dental, vision, spending accounts, life insurance, voluntary plans, as well as participation in a 401(k) plan.\n\nSystem One is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, age, national origin, disability, family care or medical leave status, genetic information, veteran status, marital status, or any other characteristic protected by applicable federal, state, or local law.",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675638000,
      "job_posted_at_datetime_utc": "2023-02-05T23:00:00.000Z",
      "job_city": "Vienna",
      "job_state": "VA",
      "job_country": "US",
      "job_latitude": 38.901222,
      "job_longitude": -77.26526,
      "job_benefits": [
        "dental_coverage",
        "retirement_savings",
        "health_insurance"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=-eyDleC8PhgAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T23:00:00.000Z",
      "job_offer_expiration_timestamp": 1678230000,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": true,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Bachelor’s degree in Information Systems, Computer Science, Engineering, or related field",
          "Hands-on experience creating automated data pipelines using modern technology stacks for batch ETL, data streaming, or change data capture, and for data processing to load advanced analytics data repositories",
          "Proficient in designing and implementing data integration processes in a large, distributed environment using cloud services e.g., Azure Event Hub, Data Factory, Data Catalog, Databricks, Stream Analytics, Apache Spark",
          "Experience in designing data lake storage structures, data acquisition, transformation, and distribution processing",
          "Proficient in developing and operationalizing various data distribution patterns such as APIs, event based, publish/subscribe models",
          "Experience migrating data lakes from on-premises to cloud",
          "Experience using the Apache Hadoop technology stack",
          "Advanced experience in SQL programming",
          "Proficient in programming languages (e.g., Python, Java)",
          "Experience with agile delivery using Jira, or Azure DevOps",
          "Experience monitoring performance of data workflows and infrastructure through use of monitoring tools and enabling alerts",
          "Functional knowledge of data visualization tools",
          "Excellent written and oral communication skills"
        ],
        "Responsibilities": [
          "This Data Engineer (Cloud Analytics) will be support data sourcing strategies, designs, and implementation for the Client Enterprise Cloud Data Lake by designing, building, and integrating data from various sources in support of predictive and adaptive analytics"
        ],
        "Benefits": [
          "Pay Rate: Open to Both C2C and W2 options",
          "System One not only serves as a valued partner for our clients, but we offer eligible employees health and welfare benefits coverage options including medical, dental, vision, spending accounts, life insurance, voluntary plans, as well as participation in a 401(k) plan"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Big Country Jobs",
      "job_id": "xmbcdLCz-rcAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.bigcountryhomepage.com/jobs/staff-data-engineer-remote-usa-salt-lake-city-utah/906556569-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612982,
      "job_posted_at_datetime_utc": "2023-02-05T16:03:02.000Z",
      "job_city": "Salt Lake City",
      "job_state": "UT",
      "job_country": "US",
      "job_latitude": 40.76078,
      "job_longitude": -111.891045,
      "job_benefits": [
        "dental_coverage",
        "health_insurance",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=xmbcdLCz-rcAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "KPMG",
      "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/KPMG_logo.svg/2560px-KPMG_logo.svg.png",
      "employer_website": "http://www.kpmg.com",
      "job_publisher": "Clearance Jobs",
      "job_id": "6YP5ITqvH0AAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Senior Specialist, Federal Data Engineer Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/7031204/senior-specialist-federal-data-engineer",
      "job_description": "Business Title:Senior Specialist, Federal Data Engineer\nRequisition Number:97172 - 47\nFunction:Advisory\n\nArea of Interest:\n\nState:MO\n\nCity:Kansas City\n\nDescription:\n\nThe KPMG Advisory practice is currently our fastest growing practice. We are seeing tremendous client demand, and looking forward we don't anticipate that slowing down. In this ever-changing market environment, our professionals must be adaptable and thrive in a collaborative, team-driven culture. At KPMG, our people are our number one priority. With a wealth of learning and career development opportunities, a world-class training facility and leading market tools, we make sure our people continue to grow both professionally and personally. If you're looking for a firm with a strong team connection where you can be your whole self, have an impact, advance your skills, deepen your experiences, and have the flexibility and access to constantly find new areas of inspiration and expand your capabilities, then consider a career in Advisory.\n\nKPMG is currently seeking an Senior Specialist for our Federal Advisory practice.\n\nResponsibilities:\n• Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act. The engagement includes assistance with quality assurance, data quality analysis and compliance reviews\n• Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings\n• Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources\n• Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports\n\nQualifications:\n• Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards\n• Bachelor's degree or Master's degree from an accredited college/university\n• Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries\n• Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job\n• Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)\n• Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)\n• Familiarity with cloud services and tools particularly AWS native services required for designing and implanting reproducible reports, big data processing, cloud database tools (RDS, Aurora, Redshift, EMR etc) is a plus\n• Basic knowledge on government health market, Affordable Care Act, public health policy is a plus\n• Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship\n\nKPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, citizenship status, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.\n\nKPMG does not currently require partners or employees to be fully vaccinated or test negative for COVID-19 in order to go to KPMG offices, client sites or KPMG events, except when mandated by federal, state or local law. In some circumstances, clients also may require proof of vaccination or testing (e.g., to go to the client site). /\n\nGL:5\n\nGF:15306",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675634924,
      "job_posted_at_datetime_utc": "2023-02-05T22:08:44.000Z",
      "job_city": "Kansas City",
      "job_state": "MO",
      "job_country": "US",
      "job_latitude": 39.099728,
      "job_longitude": -94.57857,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=6YP5ITqvH0AAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:44.000Z",
      "job_offer_expiration_timestamp": 1680739244,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards",
          "Bachelor's degree or Master's degree from an accredited college/university",
          "Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries",
          "Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job",
          "Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)",
          "Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)",
          "Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship"
        ],
        "Responsibilities": [
          "Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act",
          "The engagement includes assistance with quality assurance, data quality analysis and compliance reviews",
          "Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings",
          "Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources",
          "Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports"
        ],
        "Benefits": [
          "KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "WANE Jobs",
      "job_id": "gmCNatZ15EIAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.wane.com/jobs/staff-data-engineer-remote-usa-kansas-city-missouri/906554664-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612928,
      "job_posted_at_datetime_utc": "2023-02-05T16:02:08.000Z",
      "job_city": "Kansas City",
      "job_state": "MO",
      "job_country": "US",
      "job_latitude": 39.099728,
      "job_longitude": -94.57857,
      "job_benefits": [
        "health_insurance",
        "dental_coverage",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=gmCNatZ15EIAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Buchanan and Edwards",
      "employer_logo": null,
      "employer_website": "http://buchanan-edwards.com",
      "job_publisher": "Clearance Jobs",
      "job_id": "26BG7hIUARsAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/6695382/data-engineer",
      "job_description": "Overview\n\nWe're looking for game changers. People who love what they do and who they do it for. Providing consulting services in operational support, intelligence collection, reporting, and data governance, RenXTech is a leader in delivering solutions for high priority national security concerns. Are you ready for a change?\n\nRenXTech is seeking Data Engineer to support our client's operational requirements on a mission focused program within a challenging, dynamic setting located in the Washington Metropolitan Area.\n\nWe welcome the most challenging problems facing our national security and apply intellect, talent, and innovation to reach feasible and valuable solutions to each challenge. That means we expect our employees to bring their \"A game,\" and you have the right to expect the same from the company that employs you. We want to hear your ideas. We want to hear your concerns. We want to make sure you have what you need to do the best you can on the job and grow in your career with us over time. Almost all companies say they have a culture of inclusiveness and that they value employee input, but RenXTech puts that statement to work every day. It's time for a change. Come and experience a company that actually cares.\n\nClearance\n\nTS/SCI with poly\n\nResponsibilities\n\nSupport the design, development, deployment, and maintenance of a sophisticated big data ecosystem critical to answering key intelligence questions. Support a wide variety of data processing, data-flow, data management, data modeling, and data optimization efforts critical to sponsor's mission. Identify needs associated with database design, optimization, and implementation to store big data datasets. Orchestrate complex data flow patterns and data data enrichment analytics from a diverse and constantly growing range of data sets. Build and test solutions to address mission requirements for real-time data ingest and analysis.\n\nQualifications\n• Leverage distributed compute technologies such as Spark, Hadoop, or similar.\n• Leverage data flow management and orchestration tools such as NiFi, Airflow, or similar.\n• Leverage coding languages such as Python, Java, Spark, or similar.\n• Implement database technologies such as SQL, Mongo, or similar.\n• Evaluate, prototype, and deploy big data database technologies such as Accumulo, HBASE, Cassandra, Elastic, or similar.\n• Utilize containerization technologies such as Docker, Podman, Kubernetes, or similar.\n• Leverage mathematics, computer science, and data science expertise to support analytic design, development, and implementations to support critical mission requirements.\n• Integrate with distributed file systems such as Hadoop File System, Gluster, Ceph, or similar.\n• Leverage bucket storage technologies such as S3 or similar.\n• Evaluate, demonstrate, and deploy hot/cold storage design patterns for cost optimization.\n• Manage and test new models for data processing and data flow patterns. Work with key stakeholders to identify and remediate issues related to broken data flows.\n• Maintain awareness of emerging technologies and advancements in database design and management, data science, and machine learning.\n\nRenXTech is a wholly owned subsidiary of Buchanan & Edwards, and provides consulting services in operational support, intelligence collection, reporting, and data governance. Located in Arlington, VA, RenXTech offers expertise on a variety of national security topics, with specific focus on operational support, technical operations, policies and management of large volumes of data, analysis of large amounts of data, & effective approaches to evaluating and sharing data, and innovative techniques for information collection. Our consultants develop and deliver training programs on subjects including cyber intelligence, data governance and information sharing protocols, intelligence collection, and reporting techniques. RenXTech has a mission first culture and boasts the benefits & perks of a small company with the resources, stability, and career mobility of a large corporation. RenXTech is dedicated to fostering, cultivating and preserving a culture of diversity and inclusion. We are committed to crafting a workplace that endorses creativity and innovation, and promotes engagement through open communication, acceptance of new people and ideas, and a supportive team dynamic. RenXTech is an equal Opportunity Employer - Minorities / Women / Veterans / Individuals with Disabilities / Gender Identity / Sexual Orientation.\n\n#CJ",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675616610,
      "job_posted_at_datetime_utc": "2023-02-05T17:03:30.000Z",
      "job_city": "McLean",
      "job_state": "VA",
      "job_country": "US",
      "job_latitude": 38.93387,
      "job_longitude": -77.17726,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=26BG7hIUARsAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:30.000Z",
      "job_offer_expiration_timestamp": 1680739230,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": false,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Leverage distributed compute technologies such as Spark, Hadoop, or similar",
          "Leverage data flow management and orchestration tools such as NiFi, Airflow, or similar",
          "Leverage coding languages such as Python, Java, Spark, or similar",
          "Implement database technologies such as SQL, Mongo, or similar",
          "Evaluate, prototype, and deploy big data database technologies such as Accumulo, HBASE, Cassandra, Elastic, or similar",
          "Utilize containerization technologies such as Docker, Podman, Kubernetes, or similar",
          "Leverage mathematics, computer science, and data science expertise to support analytic design, development, and implementations to support critical mission requirements",
          "Integrate with distributed file systems such as Hadoop File System, Gluster, Ceph, or similar",
          "Leverage bucket storage technologies such as S3 or similar",
          "Evaluate, demonstrate, and deploy hot/cold storage design patterns for cost optimization"
        ],
        "Responsibilities": [
          "Support the design, development, deployment, and maintenance of a sophisticated big data ecosystem critical to answering key intelligence questions",
          "Support a wide variety of data processing, data-flow, data management, data modeling, and data optimization efforts critical to sponsor's mission",
          "Identify needs associated with database design, optimization, and implementation to store big data datasets",
          "Orchestrate complex data flow patterns and data data enrichment analytics from a diverse and constantly growing range of data sets",
          "Build and test solutions to address mission requirements for real-time data ingest and analysis",
          "Manage and test new models for data processing and data flow patterns",
          "Work with key stakeholders to identify and remediate issues related to broken data flows",
          "Maintain awareness of emerging technologies and advancements in database design and management, data science, and machine learning"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Jobs Market",
      "job_id": "vMhlIjFIvWsAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobsmarket.com/jobs/staff-data-engineer-remote-usa-houston-texas/906525243-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612442,
      "job_posted_at_datetime_utc": "2023-02-05T15:54:02.000Z",
      "job_city": "Houston",
      "job_state": "TX",
      "job_country": "US",
      "job_latitude": 29.760427,
      "job_longitude": -95.369804,
      "job_benefits": [
        "health_insurance",
        "dental_coverage",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=vMhlIjFIvWsAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T15:54:02.000Z",
      "job_offer_expiration_timestamp": 1678204442,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "STEMBoard",
      "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRnYbTu_vgIi7Dt6RC12kIN2DRKgR0pyUVRxpwi&s=0",
      "employer_website": null,
      "job_publisher": "Recruit.net",
      "job_id": "sBU-ziWtSZEAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer",
      "job_apply_link": "https://www.recruit.net/job/data-engineer-jobs/F52C82EBB954F761",
      "job_description": "SOCOM – Tampa, FL (Hybrid Schedule) – Top Secret Clearance Required\n\nPosition Overview:\n\nSTEMBoard is seeking a data engineer with an understanding of performance optimization and data pipelining. The data engineer will create and integrate application programming interfaces (APIs) and apply multiple programming languages including knowledge of SQL/NoSQL database design.\n\nThe data engineer role requires knowledge in programming for integrating complex models and using a software library frameworks to distribute large, clustered data sets. Data engineers collect and arrange data in a form that is useful for analytics. A basic knowledge in machine learning is also required to build efficient and accurate data pipelines to meet the needs for downstream users such as data scientists to create the models and analytics that produce insight.\n\nPrincipal Duties and Responsibilities:\n• Collaborate with a team of data Stewards in the development of the program and data analytics projects.\n• Developing, maintaining, and testing infrastructures for data generation to transform data from various structured and unstructured data sources.\n• Develop complex queries to ensure accessibility while optimizing the performance of NoSQL and or big data infrastructure. Create and maintain optimal data pipeline architecture.\n• Build and maintain the infrastructure to support extraction, transformation, and loading (ETL) of data from a wide variety of data sources.\n• Extract data from multiple data sources, relational SQL and NoSQL databases, and other platform APIs, for data ingestion and integration.\n• Configure and manage data analytic frameworks and pipelines using databases and tools such as NoSQL, SQL, HDInsight, MongoDB, Cassandra, Neo4j, GraphDB, OrientDB, Spark, Hadoop, Kafka, Hive, and Pig.\n• Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms.\n• Administrate cloud computing and CI/CD pipelines to include Azure, Google, and Amazon Web Service (AWS).\n• Coordinate with stakeholders, including product, data and design teams to assist with data-related technical issues and support their data infrastructure needs\n\nRequirements\n\nRequired Education/Experience:\n• Experience: 1+ years of experience with software engineering, data engineering or related experience.\n• Education: Bachelor’s in STEM with a preference towards Data Science, Computer Science, or Software Engineering.\n• Verifiable work experience working with data structures, database management, distributed computing, and API driven architectures using SQL and No-SQL engines.\n• Proficient in modeling frameworks like Universal Modeling Language (UML), Agile Development, and Git Operations.\n\nBenefits\n• Healthcare, Vision, and Dental Insurance\n• 20 Days of PTO\n• 401K Matching\n• Training/Certification Reimbursement\n• Short term/Long term disability\n• Parental/Maternity Leave\n• Life Insurance\n\nSTEMBoard is committed to hiring and retaining a diverse workforce. All qualified candidates will receive consideration for employment without regard to disability, protected veteran status, race, color, religious creed, national origin, citizenship, marital status, sex, sexual orientation/gender identity, age, or genetic information. Selected applicant will be subject to a background investigation. STEMBoard is an Equal Opportunity/Affirmative Action employer.",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675641600,
      "job_posted_at_datetime_utc": "2023-02-06T00:00:00.000Z",
      "job_city": "Tampa",
      "job_state": "FL",
      "job_country": "US",
      "job_latitude": 27.950575,
      "job_longitude": -82.45718,
      "job_benefits": [
        "dental_coverage",
        "paid_time_off",
        "health_insurance",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=sBU-ziWtSZEAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-03T00:00:00.000Z",
      "job_offer_expiration_timestamp": 1677801600,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 12,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": 70000,
      "job_max_salary": 90000,
      "job_salary_currency": "USD",
      "job_salary_period": "YEAR",
      "job_highlights": {
        "Qualifications": [
          "Experience: 1+ years of experience with software engineering, data engineering or related experience",
          "Education: Bachelor’s in STEM with a preference towards Data Science, Computer Science, or Software Engineering",
          "Verifiable work experience working with data structures, database management, distributed computing, and API driven architectures using SQL and No-SQL engines",
          "Proficient in modeling frameworks like Universal Modeling Language (UML), Agile Development, and Git Operations"
        ],
        "Responsibilities": [
          "The data engineer will create and integrate application programming interfaces (APIs) and apply multiple programming languages including knowledge of SQL/NoSQL database design",
          "The data engineer role requires knowledge in programming for integrating complex models and using a software library frameworks to distribute large, clustered data sets",
          "Data engineers collect and arrange data in a form that is useful for analytics",
          "A basic knowledge in machine learning is also required to build efficient and accurate data pipelines to meet the needs for downstream users such as data scientists to create the models and analytics that produce insight",
          "Collaborate with a team of data Stewards in the development of the program and data analytics projects",
          "Developing, maintaining, and testing infrastructures for data generation to transform data from various structured and unstructured data sources",
          "Develop complex queries to ensure accessibility while optimizing the performance of NoSQL and or big data infrastructure",
          "Create and maintain optimal data pipeline architecture",
          "Build and maintain the infrastructure to support extraction, transformation, and loading (ETL) of data from a wide variety of data sources",
          "Extract data from multiple data sources, relational SQL and NoSQL databases, and other platform APIs, for data ingestion and integration",
          "Configure and manage data analytic frameworks and pipelines using databases and tools such as NoSQL, SQL, HDInsight, MongoDB, Cassandra, Neo4j, GraphDB, OrientDB, Spark, Hadoop, Kafka, Hive, and Pig",
          "Apply distributed systems concepts and principles such as consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms",
          "Administrate cloud computing and CI/CD pipelines to include Azure, Google, and Amazon Web Service (AWS)",
          "Coordinate with stakeholders, including product, data and design teams to assist with data-related technical issues and support their data infrastructure needs"
        ],
        "Benefits": [
          "Healthcare, Vision, and Dental Insurance",
          "20 Days of PTO",
          "401K Matching",
          "Training/Certification Reimbursement",
          "Short term/Long term disability",
          "Parental/Maternity Leave",
          "Life Insurance"
        ]
      }
    },
    {
      "employer_name": "Radiance Technologies",
      "employer_logo": "https://media.licdn.com/dms/image/C4D0BAQHJyG4K1PABjA/company-logo_200_200/0/1609368764870?e=2147483647&v=beta&t=AfdBcAhEfE6r83f8k3b1zd-bnh-Sr4qn0YnVPfC8Dys",
      "employer_website": null,
      "job_publisher": "Clearance Jobs",
      "job_id": "xQaceFDhGT0AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Senior Data Engineer Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/6743013/senior-data-engineer",
      "job_description": "Job Description\n\nRadiance Technologies, a rapidly growing and 100% employee-owned company, is seeking a Data Engineer to support the analytical and technical services enabling the DCS, G-2 to successfully collaborate in the assessment, evolution, and transformation of Army Military Intelligence (MI) systems, tools, capabilities, training, and policies within the Army. Services also include providing intelligence-related planning, programming, budgeting, execution, and staff action communications in support of Army, Joint, Coalition commands and the National Intelligence Community.\n\nThis position will work alongside other Data Scientists, Data Engineers, and Intelligence Professionals to support Army intelligence policy, plans and programs necessary to collaborate in the assessment, evolution, and transformation of Army MI systems, tools, capabilities, training, and mutually supporting policies within the Army and Intelligence Community.\n\nPrincipal Responsibilities:\n• Execute and on occasion, lead research, analysis, and evaluation efforts as well as studies, analyses, assessments, and technical reports, which may include the use of existing documents, databases, models, architectures, and simulations.\n• Produce reports in multiple of formats in response to a wide range of requirements and delivery schedules.\n• Provide mid to high-level analytical assessments and advice on complex issues, which require extensive knowledge of the subject matter.\n• May attend various types of symposia and meetings at the ARSTAF and DOD level.\n• Produce and defend detailed and/or short-fused studies, presentations, information papers, and decision papers for senior DoD, DA, and IC senior leadership.\n\nRequired Skills\n• Knowledge of Joint and Army processes-JCIDS, TAA, PED, JUONS/ONS, and CONOPS.\n• Knowledge of intelligence fusion systems within each corresponding service and associated architectures\n• Knowledge of OSD, Joint and Army organization roles, missions, and functions\n• SME level of knowledge of intelligence fusion systems, capabilities / employment, training, associated R&D efforts, and program budget processes\n• Must have superior written and oral communication skills\n• Active TS/SCI security clearance\n\nRequired Experience\n• Bachelor's Degree; advanced intelligence discipline training; or other equivalent DoD or service Intelligence experience\n• Minimum Experience: 14 years of experience as an Army Intelligence analyst with experience from tactical to strategic.\n• Recent experience as a staff action officer at the HQDA (DCS, G-2 preferred) or Joint or a closely related DOD organization/agency.\n• Experience in preparation and presentation of briefings on projects, studies, and analysis to senior leaders and other ARSTAF action officers\n• Must have experience with intelligence fusion system fielding processes and schedules\n\nDesired Qualifications\n• Graduate from the Command and General Staff College or similar Senior Staff College\n\nJob Location\n\nCrystal City, Virginia, United States\n\nPosition Type\n\nFull-Time/Regular US Citizenship Required\nYes\nAbility to obtain a Security Clearance\nYes\nPolygraph Required\nNot Required",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675617874,
      "job_posted_at_datetime_utc": "2023-02-05T17:24:34.000Z",
      "job_city": "Arlington",
      "job_state": "VA",
      "job_country": "US",
      "job_latitude": 38.85011,
      "job_longitude": -77.03918,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=xQaceFDhGT0AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:34.000Z",
      "job_offer_expiration_timestamp": 1680739234,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 168,
        "experience_mentioned": true,
        "experience_preferred": true
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Knowledge of Joint and Army processes-JCIDS, TAA, PED, JUONS/ONS, and CONOPS",
          "Knowledge of intelligence fusion systems within each corresponding service and associated architectures",
          "Knowledge of OSD, Joint and Army organization roles, missions, and functions",
          "SME level of knowledge of intelligence fusion systems, capabilities / employment, training, associated R&D efforts, and program budget processes",
          "Must have superior written and oral communication skills",
          "Active TS/SCI security clearance",
          "Bachelor's Degree; advanced intelligence discipline training; or other equivalent DoD or service Intelligence experience",
          "Minimum Experience: 14 years of experience as an Army Intelligence analyst with experience from tactical to strategic",
          "Experience in preparation and presentation of briefings on projects, studies, and analysis to senior leaders and other ARSTAF action officers",
          "Must have experience with intelligence fusion system fielding processes and schedules",
          "Full-Time/Regular US Citizenship Required",
          "Ability to obtain a Security Clearance"
        ],
        "Responsibilities": [
          "Services also include providing intelligence-related planning, programming, budgeting, execution, and staff action communications in support of Army, Joint, Coalition commands and the National Intelligence Community",
          "This position will work alongside other Data Scientists, Data Engineers, and Intelligence Professionals to support Army intelligence policy, plans and programs necessary to collaborate in the assessment, evolution, and transformation of Army MI systems, tools, capabilities, training, and mutually supporting policies within the Army and Intelligence Community",
          "Execute and on occasion, lead research, analysis, and evaluation efforts as well as studies, analyses, assessments, and technical reports, which may include the use of existing documents, databases, models, architectures, and simulations",
          "Produce reports in multiple of formats in response to a wide range of requirements and delivery schedules",
          "Provide mid to high-level analytical assessments and advice on complex issues, which require extensive knowledge of the subject matter",
          "May attend various types of symposia and meetings at the ARSTAF and DOD level",
          "Produce and defend detailed and/or short-fused studies, presentations, information papers, and decision papers for senior DoD, DA, and IC senior leadership"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "KSNT Jobs",
      "job_id": "Re4siN3YDPUAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.ksnt.com/jobs/staff-data-engineer-remote-usa-portland-oregon/906542352-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612699,
      "job_posted_at_datetime_utc": "2023-02-05T15:58:19.000Z",
      "job_city": "Portland",
      "job_state": "OR",
      "job_country": "US",
      "job_latitude": 45.515232,
      "job_longitude": -122.67838,
      "job_benefits": [
        "dental_coverage",
        "health_insurance",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=Re4siN3YDPUAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "WJTV Jobs",
      "job_id": "lH4CqZvMpSoAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.wjtv.com/jobs/staff-data-engineer-remote-usa-st.-louis-missouri/906555669-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612955,
      "job_posted_at_datetime_utc": "2023-02-05T16:02:35.000Z",
      "job_city": "St. Louis",
      "job_state": "MO",
      "job_country": "US",
      "job_latitude": 38.627003,
      "job_longitude": -90.1994,
      "job_benefits": [
        "health_insurance",
        "dental_coverage",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=20&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=lH4CqZvMpSoAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Macy's",
      "employer_logo": "https://www.macys.com/img/nav/co_macysLogo3.gif",
      "employer_website": "https://www.macys.com",
      "job_publisher": "LinkedIn",
      "job_id": "qgBuZP07cWsAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Sr. Data Engineer",
      "job_apply_link": "https://www.linkedin.com/jobs/view/sr-data-engineer-at-macy-s-3421751062",
      "job_description": "Macy's, Inc is building an Enterprise Data & Analytics team to further grow our capabilities in support of our mission to be a data - led, customer centric company. This team will focus on accelerating impact from analytics, coordinating an enterprise-wide roadmap, and ensuring proper data governance and management. As a member of this team, the engineer will help lead the charge to execute on our vision to build profitable lifetime customer relationships by embedding data & analytics at the heart of everything we do.\n\nPosition Overview\n\nThe Senior Data Engineer is responsible for development and support of data products on a modern cloud-based data lake, leveraging expertise and knowledge of multiple technologies & data domains to help build a robust, scalable, and reliable data engineering platform.\n\nThe Senior Data Engineer is responsible for providing data services for enterprise-grade analytical environments, utilizing automated data pipelines at scale, and streamlining efficient data transformations for priority use cases, be involved hands on in development of the codebase and partner closely with business units and peer technology groups to support analytics execution.\n\nEssential Functions\n\nSolution Design & Implementation:\n• Collaborate with peer engineers & project managers, implement scalable solutions to meet requirements\n• Follow and improve existing processes and procedures\n• Participate within a pod of data engineers, deliver solutions independently and with peers\n• Build, maintain and simplify enterprise data pipelines with emphasis on reusability & data quality\n• Work with Legal and Privacy teams to adhere to data privacy and security requirements\n\nCulture\n• Train fellow engineers on both technical stack and data domain specifics\n• Drive change management to increase user adoption of enterprise data repositories and leverage standardized data pipelines across use cases\n• Increase agility in identifying data issues and taking action to remediate\n\nQualifications\n\nEducation/ Experience:\n• Data engineering experience with:\n• 1+ years of experience in implementing cloud-based data solutions\n• 1+ years of experience integrating with analytics reporting solutions (e.g. Tablaeu, PowerBI)\n• 3+ years of experience building & automating ETL data pipelines using enterprise grade tools\n• 3+ years of experience building enterprise-grade data warehouses (either on-prem or on cloud)\n• 5+ years of overall programming experience, including recent experience with Python & SQL\n• Ability to effectively share technical information, communicate technical issues and solutions to all levels of business stakeholders\n• Customer-centric and experienced with cross-functional collaboration\n• Excellent written and verbal communication skills\n\nWhat We Can Offer You\n• Exciting, challenging problems to solve - you'll never have a boring day at work\n• A refreshingly fun work environment where you will collaborate with a smart and talented team\n• Unique freedom to build and lead a team in next gen thinking\n• A chance to learn and participate in the growth of NYC’s largest retailer\n\nThis job description is not all inclusive. Macy’s Inc. reserves the right to amend this job description at any time. Macy's Inc. is an Equal Opportunity Employer, committed to a diverse and inclusive work environment.",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675626312,
      "job_posted_at_datetime_utc": "2023-02-05T19:45:12.000Z",
      "job_city": "Johns Creek",
      "job_state": "GA",
      "job_country": "US",
      "job_latitude": 34.028927,
      "job_longitude": -84.19858,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=qgBuZP07cWsAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T19:45:12.000Z",
      "job_offer_expiration_timestamp": 1678218312,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 12,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": true,
        "degree_mentioned": false,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Data engineering experience with:",
          "1+ years of experience in implementing cloud-based data solutions",
          "1+ years of experience integrating with analytics reporting solutions (e.g",
          "Tablaeu, PowerBI)",
          "3+ years of experience building & automating ETL data pipelines using enterprise grade tools",
          "3+ years of experience building enterprise-grade data warehouses (either on-prem or on cloud)",
          "5+ years of overall programming experience, including recent experience with Python & SQL",
          "Ability to effectively share technical information, communicate technical issues and solutions to all levels of business stakeholders",
          "Customer-centric and experienced with cross-functional collaboration",
          "Excellent written and verbal communication skills"
        ],
        "Responsibilities": [
          "This team will focus on accelerating impact from analytics, coordinating an enterprise-wide roadmap, and ensuring proper data governance and management",
          "As a member of this team, the engineer will help lead the charge to execute on our vision to build profitable lifetime customer relationships by embedding data & analytics at the heart of everything we do",
          "The Senior Data Engineer is responsible for providing data services for enterprise-grade analytical environments, utilizing automated data pipelines at scale, and streamlining efficient data transformations for priority use cases, be involved hands on in development of the codebase and partner closely with business units and peer technology groups to support analytics execution",
          "Collaborate with peer engineers & project managers, implement scalable solutions to meet requirements",
          "Follow and improve existing processes and procedures",
          "Participate within a pod of data engineers, deliver solutions independently and with peers",
          "Build, maintain and simplify enterprise data pipelines with emphasis on reusability & data quality",
          "Work with Legal and Privacy teams to adhere to data privacy and security requirements",
          "Drive change management to increase user adoption of enterprise data repositories and leverage standardized data pipelines across use cases",
          "Increase agility in identifying data issues and taking action to remediate"
        ],
        "Benefits": [
          "Exciting, challenging problems to solve - you'll never have a boring day at work",
          "A refreshingly fun work environment where you will collaborate with a smart and talented team",
          "Unique freedom to build and lead a team in next gen thinking",
          "A chance to learn and participate in the growth of NYC’s largest retailer"
        ]
      }
    },
    {
      "employer_name": "KPMG",
      "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/KPMG_logo.svg/2560px-KPMG_logo.svg.png",
      "employer_website": "http://www.kpmg.com",
      "job_publisher": "Clearance Jobs",
      "job_id": "HN5yjXLnfiEAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Senior Specialist, Federal Data Engineer Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/7031187/senior-specialist-federal-data-engineer",
      "job_description": "Business Title:Senior Specialist, Federal Data Engineer\nRequisition Number:97172 - 81\nFunction:Advisory\n\nArea of Interest:\n\nState:RI\n\nCity:Providence\n\nDescription:\n\nThe KPMG Advisory practice is currently our fastest growing practice. We are seeing tremendous client demand, and looking forward we don't anticipate that slowing down. In this ever-changing market environment, our professionals must be adaptable and thrive in a collaborative, team-driven culture. At KPMG, our people are our number one priority. With a wealth of learning and career development opportunities, a world-class training facility and leading market tools, we make sure our people continue to grow both professionally and personally. If you're looking for a firm with a strong team connection where you can be your whole self, have an impact, advance your skills, deepen your experiences, and have the flexibility and access to constantly find new areas of inspiration and expand your capabilities, then consider a career in Advisory.\n\nKPMG is currently seeking an Senior Specialist for our Federal Advisory practice.\n\nResponsibilities:\n• Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act. The engagement includes assistance with quality assurance, data quality analysis and compliance reviews\n• Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings\n• Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources\n• Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports\n\nQualifications:\n• Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards\n• Bachelor's degree or Master's degree from an accredited college/university\n• Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries\n• Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job\n• Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)\n• Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)\n• Familiarity with cloud services and tools particularly AWS native services required for designing and implanting reproducible reports, big data processing, cloud database tools (RDS, Aurora, Redshift, EMR etc) is a plus\n• Basic knowledge on government health market, Affordable Care Act, public health policy is a plus\n• Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship\n\nKPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, citizenship status, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.\n\nKPMG does not currently require partners or employees to be fully vaccinated or test negative for COVID-19 in order to go to KPMG offices, client sites or KPMG events, except when mandated by federal, state or local law. In some circumstances, clients also may require proof of vaccination or testing (e.g., to go to the client site). /\n\nGL:5\n\nGF:15306",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675634921,
      "job_posted_at_datetime_utc": "2023-02-05T22:08:41.000Z",
      "job_city": "Providence",
      "job_state": "RI",
      "job_country": "US",
      "job_latitude": 41.82399,
      "job_longitude": -71.412834,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=HN5yjXLnfiEAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:41.000Z",
      "job_offer_expiration_timestamp": 1680739241,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards",
          "Bachelor's degree or Master's degree from an accredited college/university",
          "Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries",
          "Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job",
          "Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)",
          "Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)",
          "Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship"
        ],
        "Responsibilities": [
          "Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act",
          "The engagement includes assistance with quality assurance, data quality analysis and compliance reviews",
          "Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings",
          "Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources",
          "Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports"
        ],
        "Benefits": [
          "KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "My ArkLaMiss Jobs",
      "job_id": "Qkp5subfPI4AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.myarklamiss.com/jobs/staff-data-engineer-remote-usa-san-jose-california/906542544-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612703,
      "job_posted_at_datetime_utc": "2023-02-05T15:58:23.000Z",
      "job_city": "San Jose",
      "job_state": "CA",
      "job_country": "US",
      "job_latitude": 37.35064,
      "job_longitude": -121.84988,
      "job_benefits": [
        "health_insurance",
        "dental_coverage",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=Qkp5subfPI4AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Redhorse Corporation",
      "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT8AnSe2bH6xmN2J7eIevNSm7Nwc_cenWRUK3Bl&s=0",
      "employer_website": "http://redhorsecorp.com",
      "job_publisher": "Clearance Jobs",
      "job_id": "WGr5gtoNoe0AAAAAAAAAAA==",
      "job_employment_type": "CONTRACTOR",
      "job_title": "Energy Data Engineer - Mid Level Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/6966981/energy-data-engineer-mid-level",
      "job_description": "About the Organization\n\nNow is a great time to join Redhorse Corporation. Redhorse specializes in developing and implementing creative strategies and solutions with private, state, and federal customers in the areas of cultural and environmental resources services, climate and energy change, information technology, and intelligence services. We are hiring creative, motivated, and talented people with a passion for doing what's right, what's smart, and what works.\n\nThe Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract. The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government. Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required. The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust.\n\nPrimary Duties and Responsibilities for this position include\n• Review and analyze the US Federal Government Executive Agencies electric load data\n• Develop and test modeling techniques to understand an agency's hourly load profile to inform the agency's hourly CFE procurement requirement as an off taker from a new CFE technology source.\n• Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client.\n• Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities.\n• Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services.\n• Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment.\n• Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles.\n• Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions.\n• Identify and implement scalable and efficient coding solutions.\n\nMinimum Requirements for skills, Experience, and Credentials\n• Master's Degree in Engineering, Computer Science, Mathematics or Statistics\n• Four (4) years of experience in enterprise data analysis with a preferred focus on utility data, to include advanced metering infrastructure (AMI) electricity data\n• Local to Washington D.C. metro area preferred, remote work is an option\n• Willingness to travel in support of on-site customer meetings\n• Willingness to obtain a Public Trust\n\nDesired Basic Requirements for Skills, Experience, and Credentials include\n• Knowledge of U.S. Federal Government Energy Legislation and Executive Orders (especially EO 14057)\n• Experience in or knowledge of utility interval data and electricity pricing\n\nExecutive Order Requirement:\n\nWhile currently on HOLD, Redhorse recognizes that we may need to comply with Executive Order 14042 which requires Federal Contractors to ensure all U.S. new hires be fully vaccinated for COVID-19. As required by the Executive Order, Redhorse will work in coordination with applicable contract agencies to consider requests for Reasonable Accommodations.\n\nRedhorse Corporation shall, in its discretion, modify or adjust the position to meet Redhorse's changing needs.\n\nThis job description is not a contract and may be adjusted as deemed appropriate in Redhorse's sole discretion.\n\nEOE/M/F/Vet/Disabled",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675617342,
      "job_posted_at_datetime_utc": "2023-02-05T17:15:42.000Z",
      "job_city": "Arlington",
      "job_state": "VA",
      "job_country": "US",
      "job_latitude": 38.87997,
      "job_longitude": -77.10677,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=WGr5gtoNoe0AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:42.000Z",
      "job_offer_expiration_timestamp": 1680739242,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 48,
        "experience_mentioned": true,
        "experience_preferred": true
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "The ideal candidate will live in the DC/Virginia/Maryland area and willing to undergo a background check to obtain a Public Trust",
          "Minimum Requirements for skills, Experience, and Credentials",
          "Master's Degree in Engineering, Computer Science, Mathematics or Statistics",
          "Willingness to travel in support of on-site customer meetings",
          "Willingness to obtain a Public Trust"
        ],
        "Responsibilities": [
          "The Data Engineer will support Redhorse in its execution of specific task areas related to the Federal Government Carbon Pollution Free Electricity (CFE) Technical and Strategy Support contract",
          "The program will establish technical frameworks and analytical tools and methods to inform CFE procurement decisions for the entirety of the federal government",
          "Work will be conducted primarily at Redhorse office locations (hybrid remote work) and on government client site as required",
          "Review and analyze the US Federal Government Executive Agencies electric load data",
          "Develop and test modeling techniques to understand an agency's hourly load profile to inform the agency's hourly CFE procurement requirement as an off taker from a new CFE technology source",
          "Support the configuration and ingestion of designated structured, unstructured, and semi-structured data repositories into capabilities that satisfy client requirements and support a data analytics pipeline to drive rapid delivery of functionality to the client",
          "Maintain all operational aspects of data transfers while accounting for the security posture of the underlying infrastructure and the systems and applications that are supported and monitoring the health of the environment through a variety of health tracking capabilities",
          "Automate configuration management, leverage tools, and stay current on data extract, transfer, and load (ETL) technologies and services",
          "Work under general guidance, demonstrate an initiative to develop approaches to solutions independently, review architecture, and identify areas for automation, optimization, right-sizing, and cost reduction to support the overall health of the environment",
          "Apply comprehension of data engineering-specific technologies and services, leverage expertise in databases and a variety of approaches to structuring and retrieving of data, comprehend Cloud architectural constructs, and support the establishment and maintenance of Cloud environments programmatically using vendor consoles",
          "Engage with multiple functional groups to comprehend client challenges, prototype new ideas and new technologies, help to create solutions to drive the next wave of innovation, and design, implement, schedule, test, and deploy full features and components of solutions",
          "Identify and implement scalable and efficient coding solutions"
        ]
      }
    },
    {
      "employer_name": "BICP",
      "employer_logo": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ-dK2PhHP5xvHCa7l20qNAWhssqbb9Qyd4A27X&s=0",
      "employer_website": null,
      "job_publisher": "LinkedIn",
      "job_id": "CUoKeMrY39AAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Principal Data Engineer",
      "job_apply_link": "https://www.linkedin.com/jobs/view/principal-data-engineer-at-bicp-3459760203",
      "job_description": "BICP, a leader in the delivery of innovative cloud ecosystems & advanced analytics data products for large enterprise customers, is looking to hire a Principal Data Engineer / Architect to be a thought leader in our Data Products Delivery team embedded at a Fortune 100 retail client in Beaverton, Oregon. You will lead a high visibility, greenfield data foundation implementation, working with critical business data delivering end-to-end data solutions. We are looking to hire a candidate with deep Data Engineering expertise to architect and implement a solution integrating data from multiple legacy data sources into a new cloud-based data ecosystem. This role requires a big picture thinker to provide expert advice as it relates to NextGen analytics and Big Data tools / technologies such as AWS, S3, Databricks, Snowflake, PySpark among others.\n\nYou will have the expertise to shape and deliver the best scalable data engineering solutions to business challenges. Must possess strong collaboration skills and a breadth of data delivery expertise. Candidate will work closely with peers in cross functional areas to deliver high-quality data solutions. Individual will help drive the strategy, architecture, and implementation of data engineering solutions in addition to providing technical expertise in their capability with a consultative skill set to drive business discussions from requirements to development.\n\nPrincipal Data Engineer will be involved with the technical architecture discussions and lead design and implementation of big data platforms and business analytics solutions to empower stakeholders to meet data-driven analytics and reporting needs. You will help with design and implement appropriate data architecture, data pipelines, reporting, dashboarding, data exploration, quality controls to enable self-service analytics for highly critical business data.\n\nWhat you will support:\n• Design and implement data solutions integrating data from a variety of data sources into solution comprising of AWS, S3, Databricks, Airflow etc.\n• Will work closely with product and analytics organizations, contributing to the design and implementation of data solutions.\n• Primary tasks include creation of Enterprise Data architecture landscape strategy including scope & approach, defining target state data architecture encompassing Information management, Metadata management, Data Governance, Enterprise Data Warehouse, and data models.\n• Proposes, guides, and builds data ecosystem solutions\n• Plays a leader role within the broader Data Foundation team\n• Continuously works to optimize and improve team velocity\n• Develops and shares materials and thought leadership\n\nWhat you bring to the table:\n• 3+ years hands-on with AWS and be familiar with enterprise data best practices and solutions.\n• 5+ years of experience working on ETL, data modeling, business intelligence architecture\n• 1+ years working with Databricks, Snowflake a big plus but not required\n• Demonstrated success working with stakeholders and implementing end-to-end data solutions including data model, data pipelines and Business Intelligence products to solve complex business requirements.\n• Prior experience in designing and implementing DW Architectures, OLAP technologies, Star-schema, Snowflake schema and Aggregation Techniques\n• Advanced SQL and programming experience with at least one language such as Python, Scala, Java, C++ highly desired\n\nAbout BICP\n\nBICP is a consulting firm focused on delivering innovative Analytics + Hyperscale platform solutions to our customers. With deep experience across a diverse ecosystem analytical products and cloud-based platforms we have the required product ambiguity and expertise to deliver best in breed solutions tailored to our client’s critical business data needs.\n\nBICP is an Equal Opportunity Employer and does not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class.",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675639053,
      "job_posted_at_datetime_utc": "2023-02-05T23:17:33.000Z",
      "job_city": "Beaverton",
      "job_state": "OR",
      "job_country": "US",
      "job_latitude": 45.486927,
      "job_longitude": -122.80403,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=CUoKeMrY39AAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-08-04T23:17:33.000Z",
      "job_offer_expiration_timestamp": 1691191053,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": true,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": 170000,
      "job_max_salary": 180000,
      "job_salary_currency": "USD",
      "job_salary_period": "YEAR",
      "job_highlights": {
        "Qualifications": [
          "This role requires a big picture thinker to provide expert advice as it relates to NextGen analytics and Big Data tools / technologies such as AWS, S3, Databricks, Snowflake, PySpark among others",
          "Must possess strong collaboration skills and a breadth of data delivery expertise",
          "3+ years hands-on with AWS and be familiar with enterprise data best practices and solutions",
          "5+ years of experience working on ETL, data modeling, business intelligence architecture",
          "1+ years working with Databricks, Snowflake a big plus but not required",
          "Demonstrated success working with stakeholders and implementing end-to-end data solutions including data model, data pipelines and Business Intelligence products to solve complex business requirements",
          "Prior experience in designing and implementing DW Architectures, OLAP technologies, Star-schema, Snowflake schema and Aggregation Techniques"
        ],
        "Responsibilities": [
          "You will lead a high visibility, greenfield data foundation implementation, working with critical business data delivering end-to-end data solutions",
          "Candidate will work closely with peers in cross functional areas to deliver high-quality data solutions",
          "Individual will help drive the strategy, architecture, and implementation of data engineering solutions in addition to providing technical expertise in their capability with a consultative skill set to drive business discussions from requirements to development",
          "Principal Data Engineer will be involved with the technical architecture discussions and lead design and implementation of big data platforms and business analytics solutions to empower stakeholders to meet data-driven analytics and reporting needs",
          "You will help with design and implement appropriate data architecture, data pipelines, reporting, dashboarding, data exploration, quality controls to enable self-service analytics for highly critical business data",
          "Design and implement data solutions integrating data from a variety of data sources into solution comprising of AWS, S3, Databricks, Airflow etc",
          "Will work closely with product and analytics organizations, contributing to the design and implementation of data solutions",
          "Primary tasks include creation of Enterprise Data architecture landscape strategy including scope & approach, defining target state data architecture encompassing Information management, Metadata management, Data Governance, Enterprise Data Warehouse, and data models",
          "Proposes, guides, and builds data ecosystem solutions",
          "Plays a leader role within the broader Data Foundation team",
          "Continuously works to optimize and improve team velocity",
          "Develops and shares materials and thought leadership"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "KHON2 Jobs",
      "job_id": "DmJfr0yBk3IAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.khon2.com/jobs/staff-data-engineer-remote-usa-nashville-tennessee/906554327-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612919,
      "job_posted_at_datetime_utc": "2023-02-05T16:01:59.000Z",
      "job_city": "Nashville",
      "job_state": "TN",
      "job_country": "US",
      "job_latitude": 36.113293,
      "job_longitude": -86.74696,
      "job_benefits": [
        "dental_coverage",
        "health_insurance",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=DmJfr0yBk3IAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Roche",
      "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/Hoffmann-La_Roche_logo.svg/250px-Hoffmann-La_Roche_logo.svg.png",
      "employer_website": "http://www.roche.com",
      "job_publisher": "LinkedIn",
      "job_id": "CY0jBhHkJu4AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer",
      "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-at-roche-3355040266",
      "job_description": "The Position\n\nGenentech seeks a talented and highly motivated Software Engineer to enable genomic data integration efforts at the interface of human genetics, functional genomics, molecular biology, disease model engineering, tissue and cellular profiling, and advanced computational methodology.\n\nThe role will lead the development of infrastructure that enables interoperability and comparability of data sets derived from different technologies and biological systems in the context of integrative data analysis. The successful candidate will collaborate with interdisciplinary teams of data curators, software engineers, data scientists and computational biologists to create new data products as we test new hypotheses through the novel integration of emerging data types, particularly in the domain of single-cell sequencing. The work will involve careful resource planning and project management, as well as hands-on data manipulation and implementation of data integration workflows.\n\nSuccessful candidates will meet many of the following requirements:\n• Master's degree or equivalent in software engineering, bioinformatics, biology or similar field\n• Experience with data architecture, data modeling, and schema design\n• Experience developing data transformation workflows in a high-level data science language like R or Python\n• Experience with relational and non-relational DB technologies, as well as search technologies\n• Experience working on or with a data curation team\n• Experience managing projects in an interdisciplinary domain\n• Some familiarity with ontologies, controlled vocabularies, and the notion of FAIR data\n• The ability to understand complex scientific problems and collaboratively develop ad hoc solutions in a rapidly changing environment\n• Strong technical communication skills\n• Experience with cloud architecture\n\nPreferred:\n• Deep familiarity and experience with genomics data, including single cell RNA-seq, variant calls, and the output of ChIP-seq and ATAC-seq experiments\n\nWhat to expect from us\n• A highly collaborative and dynamic research environment where we aim to advance the rate of scientific discovery using purposefully built solutions\n• Access to large data sets, samples and compute resources\n• Access to state-of-the-art technologies and pioneering research\n• Participation in seminar series featuring academic and industry scientists\n• Campus-like lifestyle with a healthy work-life balance\n• Mentored opportunities to further develop professional skills\n\nWho we are\n\nA member of the Roche Group, Genentech has been at the forefront of the biotechnology industry for more than 40 years, using human genetic information to develop novel medicines for serious and life-threatening diseases. We are a research-driven biotechnology company, whose medical innovations for cancer and other serious illnesses make a difference for patients across the globe. Please take this opportunity to learn about Genentech where we believe that our employees are our most important asset & are dedicated to remaining a great place to work.\n\nThe expected salary range for this position based on the primary location of California is $130,100 - $241,500. Actual pay will be determined based on experience, qualifications, geographic location, and other job-related factors permitted by law. A discretionary annual bonus may be available based on individual and Company performance. This position also qualifies for the benefits detailed at the link provided below.\n\nBenefits\n\n#rise\n\n#gREDInformatics\n\nWho we are\n\nA member of the Roche Group, Genentech has been at the forefront of the biotechnology industry for more than 40 years, using human genetic information to develop novel medicines for serious and life-threatening diseases. Genentech has multiple therapies on the market for cancer & other serious illnesses. Please take this opportunity to learn about Genentech where we believe that our employees are our most important asset & are dedicated to remaining a great place to work.\n\nUnless entitled to a reasonable accommodation under applicable law, employees are required to be fully vaccinated against COVID-19 in accordance with customer requirements, the company’s health and safety obligations and/or applicable law or guidelines.\n\nDiversity and Inclusion (D&I) are critical to the success of our company and our impact on society. We believe that by championing diversity of background, thought and experience, we can foster a sense of belonging and provide an environment where every employee feels valued, included, and able to contribute their best for the patients we serve. We’re focused on attracting, retaining, developing and advancing our people to their full potential by rewarding bold ways of thinking and integrating inclusive behaviors into every aspect of our work.\n\nGenentech is an equal opportunity employer, and we embrace the increasingly diverse world around us. Genentech prohibits unlawful discrimination based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin or ancestry, age, disability, marital status and veteran status. For more information about equal employment opportunity, visit our Genentech Careers Homepage.\n\nRoche is an equal opportunity employer and strictly prohibits unlawful discrimination based upon an individual's race, color, religion, gender, sexual orientation, gender identity/expression, national origin/ancestry, age, mental/physical disability, medical condition, marital status, veteran status, or any other characteristic protected by law.",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675603995,
      "job_posted_at_datetime_utc": "2023-02-05T13:33:15.000Z",
      "job_city": "South San Francisco",
      "job_state": "CA",
      "job_country": "US",
      "job_latitude": 37.654655,
      "job_longitude": -122.40775,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=CY0jBhHkJu4AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-09T19:53:24.000Z",
      "job_offer_expiration_timestamp": 1681070004,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": true,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Master's degree or equivalent in software engineering, bioinformatics, biology or similar field",
          "Experience with data architecture, data modeling, and schema design",
          "Experience developing data transformation workflows in a high-level data science language like R or Python",
          "Experience with relational and non-relational DB technologies, as well as search technologies",
          "Experience working on or with a data curation team",
          "Experience managing projects in an interdisciplinary domain",
          "Some familiarity with ontologies, controlled vocabularies, and the notion of FAIR data",
          "The ability to understand complex scientific problems and collaboratively develop ad hoc solutions in a rapidly changing environment",
          "Strong technical communication skills",
          "Experience with cloud architecture"
        ],
        "Responsibilities": [
          "The successful candidate will collaborate with interdisciplinary teams of data curators, software engineers, data scientists and computational biologists to create new data products as we test new hypotheses through the novel integration of emerging data types, particularly in the domain of single-cell sequencing",
          "The work will involve careful resource planning and project management, as well as hands-on data manipulation and implementation of data integration workflows"
        ],
        "Benefits": [
          "Access to large data sets, samples and compute resources",
          "Participation in seminar series featuring academic and industry scientists",
          "Campus-like lifestyle with a healthy work-life balance",
          "Mentored opportunities to further develop professional skills",
          "The expected salary range for this position based on the primary location of California is $130,100 - $241,500",
          "Actual pay will be determined based on experience, qualifications, geographic location, and other job-related factors permitted by law",
          "A discretionary annual bonus may be available based on individual and Company performance",
          "This position also qualifies for the benefits detailed at the link provided below"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Big Country Jobs",
      "job_id": "xrCtceUgT-gAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.bigcountryhomepage.com/jobs/staff-data-engineer-remote-usa-raleigh-north-carolina/906541135-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612677,
      "job_posted_at_datetime_utc": "2023-02-05T15:57:57.000Z",
      "job_city": "Raleigh",
      "job_state": "NC",
      "job_country": "US",
      "job_latitude": 35.77959,
      "job_longitude": -78.638176,
      "job_benefits": [
        "dental_coverage",
        "retirement_savings",
        "health_insurance"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=xrCtceUgT-gAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "KPMG",
      "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/KPMG_logo.svg/2560px-KPMG_logo.svg.png",
      "employer_website": "http://www.kpmg.com",
      "job_publisher": "Clearance Jobs",
      "job_id": "ImbO6XDF4HkAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Senior Specialist, Federal Data Engineer Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/7031197/senior-specialist-federal-data-engineer",
      "job_description": "Business Title:Senior Specialist, Federal Data Engineer\nRequisition Number:97172 - 56\nFunction:Advisory\n\nArea of Interest:\n\nState:NE\n\nCity:Omaha\n\nDescription:\n\nThe KPMG Advisory practice is currently our fastest growing practice. We are seeing tremendous client demand, and looking forward we don't anticipate that slowing down. In this ever-changing market environment, our professionals must be adaptable and thrive in a collaborative, team-driven culture. At KPMG, our people are our number one priority. With a wealth of learning and career development opportunities, a world-class training facility and leading market tools, we make sure our people continue to grow both professionally and personally. If you're looking for a firm with a strong team connection where you can be your whole self, have an impact, advance your skills, deepen your experiences, and have the flexibility and access to constantly find new areas of inspiration and expand your capabilities, then consider a career in Advisory.\n\nKPMG is currently seeking an Senior Specialist for our Federal Advisory practice.\n\nResponsibilities:\n• Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act. The engagement includes assistance with quality assurance, data quality analysis and compliance reviews\n• Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings\n• Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources\n• Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports\n\nQualifications:\n• Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards\n• Bachelor's degree or Master's degree from an accredited college/university\n• Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries\n• Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job\n• Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)\n• Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)\n• Familiarity with cloud services and tools particularly AWS native services required for designing and implanting reproducible reports, big data processing, cloud database tools (RDS, Aurora, Redshift, EMR etc) is a plus\n• Basic knowledge on government health market, Affordable Care Act, public health policy is a plus\n• Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship\n\nKPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, citizenship status, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.\n\nKPMG does not currently require partners or employees to be fully vaccinated or test negative for COVID-19 in order to go to KPMG offices, client sites or KPMG events, except when mandated by federal, state or local law. In some circumstances, clients also may require proof of vaccination or testing (e.g., to go to the client site). /\n\nGL:5\n\nGF:15306",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612912,
      "job_posted_at_datetime_utc": "2023-02-05T16:01:52.000Z",
      "job_city": "Omaha",
      "job_state": "NE",
      "job_country": "US",
      "job_latitude": 41.25654,
      "job_longitude": -95.9345,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=ImbO6XDF4HkAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:52.000Z",
      "job_offer_expiration_timestamp": 1680739252,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards",
          "Bachelor's degree or Master's degree from an accredited college/university",
          "Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries",
          "Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job",
          "Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)",
          "Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)",
          "Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship"
        ],
        "Responsibilities": [
          "Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act",
          "The engagement includes assistance with quality assurance, data quality analysis and compliance reviews",
          "Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings",
          "Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources",
          "Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports"
        ],
        "Benefits": [
          "KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package"
        ]
      }
    },
    {
      "employer_name": "Medline Industries, LP",
      "employer_logo": "https://upload.wikimedia.org/wikipedia/en/thumb/8/89/Medline-logo.svg/1200px-Medline-logo.svg.png",
      "employer_website": "http://www.medline.com",
      "job_publisher": "LinkedIn",
      "job_id": "y9w66utV5eYAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Data Engineer - Hybrid",
      "job_apply_link": "https://www.linkedin.com/jobs/view/data-engineer-hybrid-at-medline-industries-lp-3350791407",
      "job_description": "Medline has an immediate opening for a Data Engineer in our Inventory Management Department. This role would be a member of the Supply Chain Optimization team and support Network Modeling efforts to identify and implement operational cost saving initiatives. This is a great opportunity to make impactful change in a fast-paced and dynamic organization. The ideal candidate would have experience with Network Modeling, working with big data, and passion for problem solving\n\nJob Summary\n\nIdentify data needs for the Inventory Management team. Understand specific requirements for metrics and analysis, and build efficient and scalable solutions.\n\nResponsibilities\n• Designs, builds and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources\n• Establish and build processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required\n• Develop technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis\n• Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements\n• Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. May be internal or external, client-focused, working in conjunction with Professional Services and outsourcing functions. May include company-wide, web-enabled solutions\n• Own critical data pipelines and constantly improve pipeline efficiency and data quality\n• Conduct research into new data warehouse applications and determining viability for adoption\n• Assist with the design and management of our technology stack used for data storage and processing\n• Produce ideas for exploratory analysis that shape future projects and growth initiatives\n• Create dashboards and reports to regularly communicate business results to executive leadership\n• Contribute to the development and education plans on data engineering capabilities, systems, standards, and processes\n\nMinimum Requirements\n\nEducation\n• Typically requires a Bachelor’s degree in Computer Science, Math, or a related technical field\n\nWork Experience\n• Experience working in data warehousing, data architecture and/or data engineering environments at enterprise scale\n• 3+ years of overall technology experience, that includes at least 2+ years of hands-on data engineering, and data architecture\n• 2+ years of SQL experience\n• Experience working with any ETL tools\n• Experience working with relational SQL including Azure SQL Data Warehouse etc\n• Experience with version control systems like Github\n\nKnowledge/Skills/Abilities\n• Strong sense of initiative and curiosity around process automation and ability to independently make recommendations and take on projects from beginning to end\n• Ability to analyze data to identify problems, gaps and inconsistencies\n• Communication skills including the ability to identify and communicate data driven insights to senior leadership\n\nPreferred Requirements\n\nWork experience\n• 1+ year of Python programming experience.\n• Experience in Azure Data Factory will be a plus\n\nPrimary Location\n\nUS-IL-CHICAGO\n\nOther Locations\n\nUS-IL-Mundelein, US-IL-Northfield",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675603207,
      "job_posted_at_datetime_utc": "2023-02-05T13:20:07.000Z",
      "job_city": "Northfield",
      "job_state": "IL",
      "job_country": "US",
      "job_latitude": 42.09975,
      "job_longitude": -87.7809,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=30&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=y9w66utV5eYAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-09T20:40:36.000Z",
      "job_offer_expiration_timestamp": 1681072836,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 36,
        "experience_mentioned": true,
        "experience_preferred": true
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": true,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "The ideal candidate would have experience with Network Modeling, working with big data, and passion for problem solving",
          "Typically requires a Bachelor’s degree in Computer Science, Math, or a related technical field",
          "Experience working in data warehousing, data architecture and/or data engineering environments at enterprise scale",
          "3+ years of overall technology experience, that includes at least 2+ years of hands-on data engineering, and data architecture",
          "2+ years of SQL experience",
          "Experience working with any ETL tools",
          "Experience working with relational SQL including Azure SQL Data Warehouse etc",
          "Experience with version control systems like Github",
          "Strong sense of initiative and curiosity around process automation and ability to independently make recommendations and take on projects from beginning to end",
          "Ability to analyze data to identify problems, gaps and inconsistencies",
          "Communication skills including the ability to identify and communicate data driven insights to senior leadership",
          "1+ year of Python programming experience"
        ],
        "Responsibilities": [
          "This role would be a member of the Supply Chain Optimization team and support Network Modeling efforts to identify and implement operational cost saving initiatives",
          "Identify data needs for the Inventory Management team",
          "Understand specific requirements for metrics and analysis, and build efficient and scalable solutions",
          "Designs, builds and oversees the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources",
          "Establish and build processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required",
          "Develop technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis",
          "Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements",
          "Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs",
          "May be internal or external, client-focused, working in conjunction with Professional Services and outsourcing functions",
          "May include company-wide, web-enabled solutions",
          "Own critical data pipelines and constantly improve pipeline efficiency and data quality",
          "Conduct research into new data warehouse applications and determining viability for adoption",
          "Assist with the design and management of our technology stack used for data storage and processing",
          "Produce ideas for exploratory analysis that shape future projects and growth initiatives",
          "Create dashboards and reports to regularly communicate business results to executive leadership",
          "Contribute to the development and education plans on data engineering capabilities, systems, standards, and processes"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Jobs Market",
      "job_id": "iuzioZRMf-EAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobsmarket.com/jobs/staff-data-engineer-remote-usa-dallas-texas/906530352-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612518,
      "job_posted_at_datetime_utc": "2023-02-05T15:55:18.000Z",
      "job_city": "Dallas",
      "job_state": "TX",
      "job_country": "US",
      "job_latitude": 32.776665,
      "job_longitude": -96.79699,
      "job_benefits": [
        "retirement_savings",
        "dental_coverage",
        "health_insurance"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=iuzioZRMf-EAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T15:55:18.000Z",
      "job_offer_expiration_timestamp": 1678204518,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "WANE Jobs",
      "job_id": "42vXgVFeHS0AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.wane.com/jobs/staff-data-engineer-remote-usa-minneapolis-minnesota/906542045-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612693,
      "job_posted_at_datetime_utc": "2023-02-05T15:58:13.000Z",
      "job_city": "Minneapolis",
      "job_state": "MN",
      "job_country": "US",
      "job_latitude": 44.977753,
      "job_longitude": -93.26501,
      "job_benefits": [
        "dental_coverage",
        "health_insurance",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=42vXgVFeHS0AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "KPMG",
      "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/KPMG_logo.svg/2560px-KPMG_logo.svg.png",
      "employer_website": "http://www.kpmg.com",
      "job_publisher": "Clearance Jobs",
      "job_id": "Fe44Dl9uRi0AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Senior Specialist, Federal Data Engineer Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/7031206/senior-specialist-federal-data-engineer",
      "job_description": "Business Title:Senior Specialist, Federal Data Engineer\nRequisition Number:97172 - 45\nFunction:Advisory\n\nArea of Interest:\n\nState:MI\n\nCity:Grand Rapids\n\nDescription:\n\nThe KPMG Advisory practice is currently our fastest growing practice. We are seeing tremendous client demand, and looking forward we don't anticipate that slowing down. In this ever-changing market environment, our professionals must be adaptable and thrive in a collaborative, team-driven culture. At KPMG, our people are our number one priority. With a wealth of learning and career development opportunities, a world-class training facility and leading market tools, we make sure our people continue to grow both professionally and personally. If you're looking for a firm with a strong team connection where you can be your whole self, have an impact, advance your skills, deepen your experiences, and have the flexibility and access to constantly find new areas of inspiration and expand your capabilities, then consider a career in Advisory.\n\nKPMG is currently seeking an Senior Specialist for our Federal Advisory practice.\n\nResponsibilities:\n• Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act. The engagement includes assistance with quality assurance, data quality analysis and compliance reviews\n• Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings\n• Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources\n• Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports\n\nQualifications:\n• Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards\n• Bachelor's degree or Master's degree from an accredited college/university\n• Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries\n• Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job\n• Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)\n• Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)\n• Familiarity with cloud services and tools particularly AWS native services required for designing and implanting reproducible reports, big data processing, cloud database tools (RDS, Aurora, Redshift, EMR etc) is a plus\n• Basic knowledge on government health market, Affordable Care Act, public health policy is a plus\n• Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship\n\nKPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, citizenship status, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.\n\nKPMG does not currently require partners or employees to be fully vaccinated or test negative for COVID-19 in order to go to KPMG offices, client sites or KPMG events, except when mandated by federal, state or local law. In some circumstances, clients also may require proof of vaccination or testing (e.g., to go to the client site). /\n\nGL:5\n\nGF:15306",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675634924,
      "job_posted_at_datetime_utc": "2023-02-05T22:08:44.000Z",
      "job_city": "Grand Rapids",
      "job_state": "MI",
      "job_country": "US",
      "job_latitude": 42.96336,
      "job_longitude": -85.66808,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=Fe44Dl9uRi0AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:44.000Z",
      "job_offer_expiration_timestamp": 1680739244,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards",
          "Bachelor's degree or Master's degree from an accredited college/university",
          "Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries",
          "Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job",
          "Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)",
          "Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)",
          "Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship"
        ],
        "Responsibilities": [
          "Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act",
          "The engagement includes assistance with quality assurance, data quality analysis and compliance reviews",
          "Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings",
          "Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources",
          "Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports"
        ],
        "Benefits": [
          "KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "WJTV Jobs",
      "job_id": "oMk2y-G6YcQAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.wjtv.com/jobs/staff-data-engineer-remote-usa-charlotte-north-carolina/906524747-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612435,
      "job_posted_at_datetime_utc": "2023-02-05T15:53:55.000Z",
      "job_city": "Charlotte",
      "job_state": "NC",
      "job_country": "US",
      "job_latitude": 35.227085,
      "job_longitude": -80.843124,
      "job_benefits": [
        "retirement_savings",
        "health_insurance",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=oMk2y-G6YcQAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "KPMG",
      "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/KPMG_logo.svg/2560px-KPMG_logo.svg.png",
      "employer_website": "http://www.kpmg.com",
      "job_publisher": "Clearance Jobs",
      "job_id": "TAz_i-gQMe0AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Senior Specialist, Federal Data Engineer Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/7031207/senior-specialist-federal-data-engineer",
      "job_description": "Business Title:Senior Specialist, Federal Data Engineer\nRequisition Number:97172 - 44\nFunction:Advisory\n\nArea of Interest:\n\nState:MI\n\nCity:Detroit\n\nDescription:\n\nThe KPMG Advisory practice is currently our fastest growing practice. We are seeing tremendous client demand, and looking forward we don't anticipate that slowing down. In this ever-changing market environment, our professionals must be adaptable and thrive in a collaborative, team-driven culture. At KPMG, our people are our number one priority. With a wealth of learning and career development opportunities, a world-class training facility and leading market tools, we make sure our people continue to grow both professionally and personally. If you're looking for a firm with a strong team connection where you can be your whole self, have an impact, advance your skills, deepen your experiences, and have the flexibility and access to constantly find new areas of inspiration and expand your capabilities, then consider a career in Advisory.\n\nKPMG is currently seeking an Senior Specialist for our Federal Advisory practice.\n\nResponsibilities:\n• Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act. The engagement includes assistance with quality assurance, data quality analysis and compliance reviews\n• Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings\n• Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources\n• Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports\n\nQualifications:\n• Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards\n• Bachelor's degree or Master's degree from an accredited college/university\n• Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries\n• Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job\n• Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)\n• Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)\n• Familiarity with cloud services and tools particularly AWS native services required for designing and implanting reproducible reports, big data processing, cloud database tools (RDS, Aurora, Redshift, EMR etc) is a plus\n• Basic knowledge on government health market, Affordable Care Act, public health policy is a plus\n• Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship\n\nKPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, citizenship status, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.\n\nKPMG does not currently require partners or employees to be fully vaccinated or test negative for COVID-19 in order to go to KPMG offices, client sites or KPMG events, except when mandated by federal, state or local law. In some circumstances, clients also may require proof of vaccination or testing (e.g., to go to the client site). /\n\nGL:5\n\nGF:15306",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675634925,
      "job_posted_at_datetime_utc": "2023-02-05T22:08:45.000Z",
      "job_city": "Detroit",
      "job_state": "MI",
      "job_country": "US",
      "job_latitude": 42.33143,
      "job_longitude": -83.04575,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=TAz_i-gQMe0AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:45.000Z",
      "job_offer_expiration_timestamp": 1680739245,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards",
          "Bachelor's degree or Master's degree from an accredited college/university",
          "Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries",
          "Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job",
          "Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)",
          "Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)",
          "Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship"
        ],
        "Responsibilities": [
          "Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act",
          "The engagement includes assistance with quality assurance, data quality analysis and compliance reviews",
          "Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings",
          "Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources",
          "Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports"
        ],
        "Benefits": [
          "KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "KHON2 Jobs",
      "job_id": "xz7PrXKDiukAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.khon2.com/jobs/staff-data-engineer-remote-usa-kansas-city-missouri/906554664-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612928,
      "job_posted_at_datetime_utc": "2023-02-05T16:02:08.000Z",
      "job_city": "Kansas City",
      "job_state": "MO",
      "job_country": "US",
      "job_latitude": 39.099728,
      "job_longitude": -94.57857,
      "job_benefits": [
        "retirement_savings",
        "health_insurance",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=xz7PrXKDiukAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Jobs Market",
      "job_id": "O7hzdiBCbWkAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobsmarket.com/jobs/staff-data-engineer-remote-usa-san-diego-california/906521440-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612386,
      "job_posted_at_datetime_utc": "2023-02-05T15:53:06.000Z",
      "job_city": "San Diego",
      "job_state": "CA",
      "job_country": "US",
      "job_latitude": 32.715736,
      "job_longitude": -117.16109,
      "job_benefits": [
        "health_insurance",
        "retirement_savings",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=O7hzdiBCbWkAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T15:53:06.000Z",
      "job_offer_expiration_timestamp": 1678204386,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "KPMG",
      "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/KPMG_logo.svg/2560px-KPMG_logo.svg.png",
      "employer_website": "http://www.kpmg.com",
      "job_publisher": "Clearance Jobs",
      "job_id": "DE_5iR7O8QEAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Senior Specialist, Federal Data Engineer Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/7031205/senior-specialist-federal-data-engineer",
      "job_description": "Business Title:Senior Specialist, Federal Data Engineer\nRequisition Number:97172 - 46\nFunction:Advisory\n\nArea of Interest:\n\nState:MN\n\nCity:Minneapolis\n\nDescription:\n\nThe KPMG Advisory practice is currently our fastest growing practice. We are seeing tremendous client demand, and looking forward we don't anticipate that slowing down. In this ever-changing market environment, our professionals must be adaptable and thrive in a collaborative, team-driven culture. At KPMG, our people are our number one priority. With a wealth of learning and career development opportunities, a world-class training facility and leading market tools, we make sure our people continue to grow both professionally and personally. If you're looking for a firm with a strong team connection where you can be your whole self, have an impact, advance your skills, deepen your experiences, and have the flexibility and access to constantly find new areas of inspiration and expand your capabilities, then consider a career in Advisory.\n\nKPMG is currently seeking an Senior Specialist for our Federal Advisory practice.\n\nResponsibilities:\n• Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act. The engagement includes assistance with quality assurance, data quality analysis and compliance reviews\n• Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings\n• Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources\n• Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports\n\nQualifications:\n• Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards\n• Bachelor's degree or Master's degree from an accredited college/university\n• Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries\n• Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job\n• Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)\n• Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)\n• Familiarity with cloud services and tools particularly AWS native services required for designing and implanting reproducible reports, big data processing, cloud database tools (RDS, Aurora, Redshift, EMR etc) is a plus\n• Basic knowledge on government health market, Affordable Care Act, public health policy is a plus\n• Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship\n\nKPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, citizenship status, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.\n\nKPMG does not currently require partners or employees to be fully vaccinated or test negative for COVID-19 in order to go to KPMG offices, client sites or KPMG events, except when mandated by federal, state or local law. In some circumstances, clients also may require proof of vaccination or testing (e.g., to go to the client site). /\n\nGL:5\n\nGF:15306",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675634924,
      "job_posted_at_datetime_utc": "2023-02-05T22:08:44.000Z",
      "job_city": "Minneapolis",
      "job_state": "MN",
      "job_country": "US",
      "job_latitude": 44.977753,
      "job_longitude": -93.26501,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=DE_5iR7O8QEAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:44.000Z",
      "job_offer_expiration_timestamp": 1680739244,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards",
          "Bachelor's degree or Master's degree from an accredited college/university",
          "Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries",
          "Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job",
          "Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)",
          "Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)",
          "Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship"
        ],
        "Responsibilities": [
          "Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act",
          "The engagement includes assistance with quality assurance, data quality analysis and compliance reviews",
          "Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings",
          "Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources",
          "Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports"
        ],
        "Benefits": [
          "KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "WANE Jobs",
      "job_id": "9nZ4ooqIPQcAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.wane.com/jobs/staff-data-engineer-remote-usa-columbus-ohio/906542078-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612694,
      "job_posted_at_datetime_utc": "2023-02-05T15:58:14.000Z",
      "job_city": "Columbus",
      "job_state": "OH",
      "job_country": "US",
      "job_latitude": 39.961174,
      "job_longitude": -82.998795,
      "job_benefits": [
        "health_insurance",
        "retirement_savings",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=9nZ4ooqIPQcAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "KPMG",
      "employer_logo": "https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/KPMG_logo.svg/2560px-KPMG_logo.svg.png",
      "employer_website": "http://www.kpmg.com",
      "job_publisher": "Clearance Jobs",
      "job_id": "RYa69z3jDt0AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Senior Specialist, Federal Data Engineer Jobs",
      "job_apply_link": "https://www.clearancejobs.com/jobs/7031173/senior-specialist-federal-data-engineer",
      "job_description": "Business Title:Senior Specialist, Federal Data Engineer\nRequisition Number:97172 - 98\nFunction:Advisory\n\nArea of Interest:\n\nState:VA\n\nCity:Norfolk\n\nDescription:\n\nThe KPMG Advisory practice is currently our fastest growing practice. We are seeing tremendous client demand, and looking forward we don't anticipate that slowing down. In this ever-changing market environment, our professionals must be adaptable and thrive in a collaborative, team-driven culture. At KPMG, our people are our number one priority. With a wealth of learning and career development opportunities, a world-class training facility and leading market tools, we make sure our people continue to grow both professionally and personally. If you're looking for a firm with a strong team connection where you can be your whole self, have an impact, advance your skills, deepen your experiences, and have the flexibility and access to constantly find new areas of inspiration and expand your capabilities, then consider a career in Advisory.\n\nKPMG is currently seeking an Senior Specialist for our Federal Advisory practice.\n\nResponsibilities:\n• Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act. The engagement includes assistance with quality assurance, data quality analysis and compliance reviews\n• Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings\n• Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources\n• Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports\n\nQualifications:\n• Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards\n• Bachelor's degree or Master's degree from an accredited college/university\n• Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries\n• Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job\n• Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)\n• Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)\n• Familiarity with cloud services and tools particularly AWS native services required for designing and implanting reproducible reports, big data processing, cloud database tools (RDS, Aurora, Redshift, EMR etc) is a plus\n• Basic knowledge on government health market, Affordable Care Act, public health policy is a plus\n• Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship\n\nKPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, citizenship status, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.\n\nKPMG does not currently require partners or employees to be fully vaccinated or test negative for COVID-19 in order to go to KPMG offices, client sites or KPMG events, except when mandated by federal, state or local law. In some circumstances, clients also may require proof of vaccination or testing (e.g., to go to the client site). /\n\nGL:5\n\nGF:15306",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675634918,
      "job_posted_at_datetime_utc": "2023-02-05T22:08:38.000Z",
      "job_city": "Norfolk",
      "job_state": "VA",
      "job_country": "US",
      "job_latitude": 36.85077,
      "job_longitude": -76.28587,
      "job_benefits": null,
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=40&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=RYa69z3jDt0AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-04-06T00:00:38.000Z",
      "job_offer_expiration_timestamp": 1680739238,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": null,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": false,
        "professional_certification_mentioned": false
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "Minimum three years of experience in developing high-quality scripts and queries, data pipelines, ETL, visualizations, and visualization dashboards",
          "Bachelor's degree or Master's degree from an accredited college/university",
          "Proficiency with programming languages such as Python, Scala and R and their open source packages/libraries",
          "Market-leading fluency in Oracle databases, Linux/Unix tools, Shell Script/Bash and Job schedulers such as Cron job",
          "Expertise with development tools and team methodologies (Agile, Git, test driven environment, CI/CD release management)",
          "Experience with large-scale big data methods (Hadoop stack, Cloud, Kubernetes)",
          "Ability to obtain a U.S. Federal government security clearance within a reasonable period of time, which requires U.S.citizenship"
        ],
        "Responsibilities": [
          "Collaborate with multi-disciplinary and cross-functional tams to provide data analytics support for our federal client in the development and operation of program integrity and financial oversight functions of the government marketplaces and marketplace-related premium stabilization programs established under the Affordable Care Act",
          "The engagement includes assistance with quality assurance, data quality analysis and compliance reviews",
          "Work with clients and team leads to discover data sources in client cloud environment, create data requests; utilize the ETL process to ingest and enrich structured and unstructured data; design, implement, ad test data processing pipelines, and data mining/data science algorithms on a variety of hosted settings",
          "Rapidly learn and understand client business processes by reviewing technical documentation, business catalogs, white papers and deep dive into KPMG's client data to better understand the relationship between datasets, client systems and data sources",
          "Contribute in developing dashboards, UI and interactive tools to support clients to turn their data into actionable insights and reproducible reports"
        ],
        "Benefits": [
          "KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "KHON2 Jobs",
      "job_id": "1bOV1RREV58AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.khon2.com/jobs/staff-data-engineer-remote-usa-denver-colorado/906542077-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612694,
      "job_posted_at_datetime_utc": "2023-02-05T15:58:14.000Z",
      "job_city": "Denver",
      "job_state": "CO",
      "job_country": "US",
      "job_latitude": 39.739235,
      "job_longitude": -104.99025,
      "job_benefits": [
        "retirement_savings",
        "health_insurance",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=50&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=1bOV1RREV58AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Jobs Market",
      "job_id": "82bOACngkacAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobsmarket.com/jobs/staff-data-engineer-remote-usa-columbus-ohio/906542078-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612694,
      "job_posted_at_datetime_utc": "2023-02-05T15:58:14.000Z",
      "job_city": "Columbus",
      "job_state": "OH",
      "job_country": "US",
      "job_latitude": 39.961174,
      "job_longitude": -82.998795,
      "job_benefits": [
        "retirement_savings",
        "dental_coverage",
        "health_insurance"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=50&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=82bOACngkacAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T15:58:14.000Z",
      "job_offer_expiration_timestamp": 1678204694,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "WANE Jobs",
      "job_id": "Eaf5GVbDGsIAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobs.wane.com/jobs/staff-data-engineer-remote-usa-san-diego-california/906521440-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612386,
      "job_posted_at_datetime_utc": "2023-02-05T15:53:06.000Z",
      "job_city": "San Diego",
      "job_state": "CA",
      "job_country": "US",
      "job_latitude": 32.715736,
      "job_longitude": -117.16109,
      "job_benefits": [
        "retirement_savings",
        "dental_coverage",
        "health_insurance"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=50&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=Eaf5GVbDGsIAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": null,
      "job_offer_expiration_timestamp": null,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Jobs Market",
      "job_id": "Z5TGIYQAn08AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobsmarket.com/jobs/staff-data-engineer-remote-usa-minneapolis-minnesota/906542045-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612693,
      "job_posted_at_datetime_utc": "2023-02-05T15:58:13.000Z",
      "job_city": "Minneapolis",
      "job_state": "MN",
      "job_country": "US",
      "job_latitude": 44.93953,
      "job_longitude": -93.279755,
      "job_benefits": [
        "health_insurance",
        "dental_coverage",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=50&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=Z5TGIYQAn08AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T15:58:13.000Z",
      "job_offer_expiration_timestamp": 1678204693,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Jobs Market",
      "job_id": "Afy-cPPBvaMAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobsmarket.com/jobs/staff-data-engineer-remote-usa-los-angeles-california/906525389-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612444,
      "job_posted_at_datetime_utc": "2023-02-05T15:54:04.000Z",
      "job_city": "Los Angeles",
      "job_state": "CA",
      "job_country": "US",
      "job_latitude": 34.052235,
      "job_longitude": -118.24368,
      "job_benefits": [
        "health_insurance",
        "retirement_savings",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=50&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=Afy-cPPBvaMAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T15:54:04.000Z",
      "job_offer_expiration_timestamp": 1678204444,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Jobs Market",
      "job_id": "o94Af_LjWs0AAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobsmarket.com/jobs/staff-data-engineer-remote-usa-phoenix-arizona/906524925-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612437,
      "job_posted_at_datetime_utc": "2023-02-05T15:53:57.000Z",
      "job_city": "Phoenix",
      "job_state": "AZ",
      "job_country": "US",
      "job_latitude": 33.448376,
      "job_longitude": -112.074036,
      "job_benefits": [
        "health_insurance",
        "retirement_savings",
        "dental_coverage"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=50&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=o94Af_LjWs0AAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T15:53:57.000Z",
      "job_offer_expiration_timestamp": 1678204437,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Jobs Market",
      "job_id": "akSYj0R8RMsAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobsmarket.com/jobs/staff-data-engineer-remote-usa-charlotte-north-carolina/906524747-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612435,
      "job_posted_at_datetime_utc": "2023-02-05T15:53:55.000Z",
      "job_city": "Charlotte",
      "job_state": "NC",
      "job_country": "US",
      "job_latitude": 35.18684,
      "job_longitude": -80.75217,
      "job_benefits": [
        "dental_coverage",
        "retirement_savings",
        "health_insurance"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=50&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=akSYj0R8RMsAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T15:53:55.000Z",
      "job_offer_expiration_timestamp": 1678204435,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    },
    {
      "employer_name": "Digimarc",
      "employer_logo": null,
      "employer_website": "http://www.digimarc.com",
      "job_publisher": "Jobs Market",
      "job_id": "MdeKjhHNIFsAAAAAAAAAAA==",
      "job_employment_type": "FULLTIME",
      "job_title": "Staff Data Engineer (Remote, USA)",
      "job_apply_link": "https://jobsmarket.com/jobs/staff-data-engineer-remote-usa-new-york/906526509-2/",
      "job_description": "Do you innovate with integrity? Believe a team is stronger together? Are you passionate about sustainability? If these are some of the attributes that describe you, then read on. We are looking for courageous, collaborative, and curious problem solvers who are interested in joining our team to help us develop solutions to complex challenges.\n\nDigimarc is a global leader in product digitization, delivering business value across industries through unique identifiers and cloud-based solutions. A trusted partner in deterring digital counterfeiting of global currency for more than 20 years, Digimarc illuminates a product's journey to provide intelligence and promote a prosperous, safer, and more sustainable world. Digimarc also gives every product a digital presence connected to a cloud-based record of its journey and interactions via the EVRYTHNG Product Cloud®. With Digimarc, you can finally see everything. And when you see everything, you can achieve anything. For more information, visit us at .\n\nTHE CHALLENGE\n\nWe are looking for a Staff Data Engineer to join our Data Engineering team who has extensive experience with data engineering and expertise in on-prem and cloud data lakes and warehouses. You will be a part of the exciting journey as a critical contributor in the transition to our next-generation cloud-based unified data platform, working with internal and external teams. The Data Engineering team is a data product delivery team dedicated to unlocking the value of Digimarc's data assets. We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues. Collaboration and clear communication with multiple teams will be critical to the success of this new data platform. The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform.\n\nWHAT YOU WILL DO\n• Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles.\n• Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)\n• Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets\n• Develop data pipelines and services to support analytic products\n• Document designs and specifications for your work\n• Follow and improve source control procedures and help establish new patterns and processes.\n• Support and contribute to DevOps and CI/CD processes\n• Design and build a data warehouse on the snowflake platform.\n\nWHAT WE ARE LOOKING FOR\n• BS Computer Science, or relevant combination of education and experience.\n• 5+ years of data engineering experience and data migration experience.\n• Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles\n• Knowledge of data warehouse data modeling\n• Experience in developing a strong quality culture where the whole team is responsible for quality\n• Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc.\n• Experience building cloud-based data warehouse solutions using Snowflake\n• Solid understanding of Agile development methodologies, quality, and CI/CD\n• Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc.\n• Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential\n• Alignment with the core Digimarc values: collaborative, curious, and courageous\n• A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team\n\nBONUS POINTS FOR\n• Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure\n• Experience working with teams to set up registries for streaming data\n\nBENEFITS\n• Competitive salary\n• Restricted stock units\n• Comprehensive benefits including medical, dental, vision, & retirement savings plan\n• Flexible time off / holidays\n• Mentorship opportunities\n\nA reasonable estimate of the current range is 140k-$175K. This compensation range considers the wide range of factors that are considered in making compensation decisions including but not limited to skill set, experience, training, licensure, certifications, geography and other business and organizational needs. The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the position may be filled.\n\nDigimarc is seeking diverse applicants. We are an equal opportunity employer and consider qualified applicants for employment without regard to race, gender, age, color, religion, disability, veteran status, sexual orientation, gender identity, or any other protected factor. We want the best people who share our values.\n\nThis job posting is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nPrincipals only. No recruiters please.\n\n#LI-remote",
      "job_is_remote": false,
      "job_posted_at_timestamp": 1675612462,
      "job_posted_at_datetime_utc": "2023-02-05T15:54:22.000Z",
      "job_city": "New York",
      "job_state": "NY",
      "job_country": "US",
      "job_latitude": 40.712776,
      "job_longitude": -74.005974,
      "job_benefits": [
        "health_insurance",
        "dental_coverage",
        "retirement_savings"
      ],
      "job_google_link": "https://www.google.com/search?gl=us&hl=en&rciv=jb&q=%22data+engineer%22+in+usa&start=50&chips=date_posted:today&schips=date_posted;today&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=%22data+engineer%22+in+usa&htidocid=MdeKjhHNIFsAAAAAAAAAAA%3D%3D",
      "job_offer_expiration_datetime_utc": "2023-03-07T15:54:22.000Z",
      "job_offer_expiration_timestamp": 1678204462,
      "job_required_experience": {
        "no_experience_required": false,
        "required_experience_in_months": 60,
        "experience_mentioned": true,
        "experience_preferred": false
      },
      "job_required_skills": null,
      "job_required_education": {
        "postgraduate_degree": false,
        "professional_certification": false,
        "high_school": false,
        "associates_degree": false,
        "bachelors_degree": false,
        "degree_mentioned": true,
        "degree_preferred": true,
        "professional_certification_mentioned": true
      },
      "job_experience_in_place_of_education": false,
      "job_min_salary": null,
      "job_max_salary": null,
      "job_salary_currency": null,
      "job_salary_period": null,
      "job_highlights": {
        "Qualifications": [
          "BS Computer Science, or relevant combination of education and experience",
          "5+ years of data engineering experience and data migration experience",
          "Experience connecting software systems to databases and cloud systems, and knowledge of data modeling principles",
          "Knowledge of data warehouse data modeling",
          "Experience in developing a strong quality culture where the whole team is responsible for quality",
          "Expert-level SQL skills, including building and maintaining views, functions, stored procedures, etc",
          "Experience building cloud-based data warehouse solutions using Snowflake",
          "Solid understanding of Agile development methodologies, quality, and CI/CD",
          "Intermediate and hands-on experience with at least one other data engineering language such as Python, R, etc",
          "Passion for participating, creating, and sustaining a diverse and inclusive culture where individuals and teams can do their best work and unlock their potential",
          "Alignment with the core Digimarc values: collaborative, curious, and courageous",
          "A positive attitude and growth mindset: the eagerness to learn & share knowledge with the team",
          "Experience building cloud-based solutions using AWS, GCP, Snowflake, or Azure",
          "Experience working with teams to set up registries for streaming data"
        ],
        "Responsibilities": [
          "We will partner with Digimarc's product and research and development teams to deliver data lakes, data warehouses, pipelines, platforms, visualizations, dashboards, reports, and other data products for our customers, partners, and colleagues",
          "Collaboration and clear communication with multiple teams will be critical to the success of this new data platform",
          "The primary duties of this role include assessing the current state and assisting and mentoring data engineers in developing, configuring, and delivering data pipelines on a cloud platform",
          "Act as a mentor on the Data Engineering team to support data engineers in understanding and following best data engineering principles",
          "Partner with software and quality engineers, product owners, as well as product managers, to assist in designing, implementing, and maintaining a new unified data platform (data lake, data warehouse, etc.)",
          "Gather an inventory of all existing data sources and planning for best storage and future usage of key data sets",
          "Develop data pipelines and services to support analytic products",
          "Document designs and specifications for your work",
          "Follow and improve source control procedures and help establish new patterns and processes",
          "Support and contribute to DevOps and CI/CD processes",
          "Design and build a data warehouse on the snowflake platform"
        ],
        "Benefits": [
          "Competitive salary",
          "Restricted stock units",
          "Comprehensive benefits including medical, dental, vision, & retirement savings plan",
          "Flexible time off / holidays",
          "Mentorship opportunities",
          "A reasonable estimate of the current range is 140k-$175K"
        ]
      }
    }
  ]
}